{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c01cea4-b8a0-49f0-91aa-3d1c11ea3b6c",
   "metadata": {},
   "source": [
    "#### ============  Ashesi University\n",
    "#### ============  Department of Computer Science and Information Systems\n",
    "#### ============  \"A Novel Cascading Method for Threats Detection Using Deep Learning Models\"\n",
    "#### ============  Clovis Mushagalusa CIRUBAKADERHA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9e7090-cf00-4c87-a102-d38debc3ef07",
   "metadata": {},
   "source": [
    "### =================  Importing Necessary Libraries  ================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772316e5-9ddf-426b-b6c2-89df90ee2f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "from absl import logging\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "logging.set_verbosity(logging.ERROR)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62e74ee-26e9-4f3e-b206-11d907c18fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import csv\n",
    "import torch\n",
    "import joblib\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from datetime import datetime\n",
    "from itertools import permutations\n",
    "from sklearn.metrics import (accuracy_score, f1_score, precision_score, recall_score, \n",
    "                            confusion_matrix, classification_report, roc_curve, auc, \n",
    "                            roc_auc_score, top_k_accuracy_score, ConfusionMatrixDisplay)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from scipy.stats import wilcoxon\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from typing import List, Tuple, Union, Optional\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ae4e23-ddac-4252-a927-73ff1d8f08a7",
   "metadata": {},
   "source": [
    "### ================  Exploratory Data Analysis (EDA)  ================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab00c246-3854-4d3f-852b-f1971e2b18ce",
   "metadata": {},
   "source": [
    "==================================  Loading the dataset  ================================== "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c8ff90-d931-4b22-ac4f-6ae01099033a",
   "metadata": {},
   "outputs": [],
   "source": [
    "drdos_dns = pd.read_csv(\"DrDoS_DNS.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3a1ede-5f31-47b3-b168-db4e32bc5db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "drdos_ldap = pd.read_csv(\"DrDoS_LDAP.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789deb51-51db-462e-a4dc-173ee132fc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "drdos_mssql = pd.read_csv(\"DrDoS_MSSQL.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4217b0a-5d3d-4ef3-9c68-a96ab917e6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "drdos_ntp = pd.read_csv(\"DrDoS_NTP.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6097b234-0fb9-4d68-9748-c5c20bd38d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "drdos_ssdp = pd.read_csv(\"DrDoS_SSDP.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276462e4-6e87-458f-9151-b4302053ac06",
   "metadata": {},
   "outputs": [],
   "source": [
    "drdos_udp = pd.read_csv(\"DrDoS_UDP.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65294b94-4e1f-48c6-8aa4-e3b605dd8863",
   "metadata": {},
   "outputs": [],
   "source": [
    "mssql = pd.read_csv(\"MSSQL.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faba82c4-5139-438f-afc7-9025935a4bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "netbios = pd.read_csv(\"NetBIOS.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b874d9f-fe94-4c8f-bd88-44ab0506ca2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "portmap = pd.read_csv(\"Portmap.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0858037-4485-4375-a379-0cb9db8ab659",
   "metadata": {},
   "outputs": [],
   "source": [
    "syn = pd.read_csv(\"Syn.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e541665-e13f-41dd-833b-5b1ce9ec91bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "udp = pd.read_csv(\"UDP.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ebb13d-dffb-4b23-9566-d7484ba39646",
   "metadata": {},
   "outputs": [],
   "source": [
    "udplag = pd.read_csv(\"UDPLag.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d81286-8e71-4d4f-9d95-116acd61f550",
   "metadata": {},
   "source": [
    "=================================  Processing the dataset  ================================= "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96eb481d-e4c3-478e-be01-78eb7426d87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_full_datasets = [\"drdos_dns\", \"drdos_ldap\", \"drdos_mssql\", \"drdos_ntp\", \"drdos_ssdp\", \"drdos_udp\", \"mssql\", \"netbios\", \"portmap\", \"syn\", \"udp\", \"udplag\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630fd68c-384f-4758-b3ad-57d1b6185df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stripping the dataset features\n",
    "\n",
    "def strip_column_names(dataset):\n",
    "    \"\"\"\n",
    "    Strips leading/trailing whitespace from column names of a DataFrame.\n",
    "    \"\"\"\n",
    "    dataset.columns = dataset.columns.str.strip()\n",
    "    return dataset\n",
    "\n",
    "# Looping through the list and updating each dataset in globals()\n",
    "\n",
    "for data in list_of_full_datasets:\n",
    "    if data in globals():\n",
    "        globals()[data] = strip_column_names(globals()[data])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002ed3d5-1c95-4330-89fd-91a2a5c093e9",
   "metadata": {},
   "source": [
    "=======================  Handling the missing and infinite values in the datasets  ======================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6fe670-54e2-4ced-84e0-059841b33ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_missing_and_infinite_values(dataset):\n",
    "    \"\"\"\n",
    "    Fills missing and infinite values:\n",
    "    - Numerical (int/float): with mean\n",
    "    - Categorical (object/string): with mode (most frequent value)\n",
    "    \"\"\"\n",
    "    for feature in dataset.columns:\n",
    "        \n",
    "        # Replacing infinite values with NaN first\n",
    "        \n",
    "        dataset[feature].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "        if dataset[feature].isnull().any():\n",
    "            if dataset[feature].dtype in ['int64', 'float64']:\n",
    "                mean_value = dataset[feature].mean()\n",
    "                dataset[feature].fillna(mean_value, inplace=True)\n",
    "            else:\n",
    "                if not dataset[feature].mode().empty:\n",
    "                    mode_value = dataset[feature].mode()[0]\n",
    "                    dataset[feature].fillna(mode_value, inplace=True)\n",
    "    return dataset\n",
    "\n",
    "# Applying the function to each dataset\n",
    "\n",
    "for data in list_of_full_datasets:\n",
    "    if data in globals():\n",
    "        print(f\"âœ… Handled ..... {data}\")\n",
    "        globals()[data] = handle_missing_and_infinite_values(globals()[data])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd11a5ae-2940-4b7a-ae57-406869082da3",
   "metadata": {},
   "source": [
    "======================  Creating a combined dataset for a better model building  ======================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2d0b96-71ea-4f86-9282-dd8b3df79def",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_a_combined_threats_dataset(datasets_names: list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Combines multiple datasets into one DataFrame.\n",
    "\n",
    "    Arguments:\n",
    "        datasets_names (list): List of variable names (as strings)\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Concatenated dataset\n",
    "    \"\"\"\n",
    "    combined_data = []\n",
    "\n",
    "    for name in datasets_names:\n",
    "        if name in globals():\n",
    "            dataframe = globals()[name]\n",
    "            if isinstance(dataframe, pd.DataFrame):\n",
    "                combined_data.append(dataframe)\n",
    "            else:\n",
    "                print(f\"âš ï¸ Variable '{name}' is not a DataFrame. Skipping. âš ï¸\")\n",
    "        else:\n",
    "            print(f\"âŒ Dataset '{name}' not found in global scope. âŒ\")\n",
    "\n",
    "    if not combined_data:\n",
    "        raise ValueError(\"No valid datasets found in the list.\")\n",
    "\n",
    "    final_dataset = pd.concat(combined_data, axis=0, ignore_index=True)\n",
    "    \n",
    "    print(f\"âœ… The new dataset shape: {final_dataset.shape}\")\n",
    "    \n",
    "    return final_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d072f7c-0638-444a-a671-45ac1b3aabbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_threats_dataset = create_a_combined_threats_dataset(list_of_full_datasets) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f668689-5fe5-474f-8e13-9f4cb51e2b09",
   "metadata": {},
   "source": [
    "================================  Checking the new datasets  ================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebd527a-04a3-4e43-872b-0f3f51dcaafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_threats_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d1fc56-9d23-4abf-a549-d60bef99783f",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_threats_dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c371f8-0b94-473b-988f-416ca5e77bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_unique_labels = combined_threats_dataset[\"Label\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1422ff83-a468-492d-8f8b-67d50b6a6430",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"âœ… We have {number_of_unique_labels} unique labels in the combined dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e7d321-6d0d-4da0-b0d6-ef351a819220",
   "metadata": {},
   "source": [
    "==============================  Threats to be used for modeling  =============================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67d5f99-a502-4683-856c-fd6425e0fdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_modeling_threats = [\"DrDoS_DNS\", \"DrDoS_NTP\", \"Syn\", \"UDP\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e463bf-a15c-4f69-bb98-a89d2772e08e",
   "metadata": {},
   "source": [
    "=================================  Reducing the dataset  ================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8adb2d-3138-4b5b-91ce-e3e8a1e684e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_dataset(data: pd.DataFrame, label_column: str = \"Label\", threats: list = None, max_per_threat: int = 500_000):\n",
    "    \"\"\"\n",
    "    Creates a dataset by retaining all BENIGN samples and sampling up to max_per_threat \n",
    "    samples for each class in the 'threats' list.\n",
    "\n",
    "    Arguments:\n",
    "        data (pd.DataFrame): The whole dataset containing all classes.\n",
    "        label_column (str): The name of the label column.\n",
    "        threats (list): List of target threat labels to include.\n",
    "        max_per_threat (int): Number of samples to retain per threat label.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Filtered and balanced dataset.\n",
    "    \"\"\"\n",
    "    if label_column not in data.columns:\n",
    "        raise KeyError(f\"âŒ The column '{label_column}' was not found in the dataset. âŒ\")\n",
    "    \n",
    "    if threats is None or len(threats) == 0:\n",
    "        raise ValueError(\"Please provide a non-empty list of threat labels to include.\")\n",
    "    \n",
    "    # Keeping all BENIGN samples\n",
    "    \n",
    "    benign_data = data[data[label_column] == \"BENIGN\"]\n",
    "\n",
    "    # Selecting and sampling only the specified threat classes\n",
    "    \n",
    "    sampled_threats = []\n",
    "    for threat in threats:\n",
    "        threat_subset = data[data[label_column] == threat]\n",
    "        if threat_subset.empty:\n",
    "            print(f\"âš ï¸ Warning: No data found for threat '{threat}' âš ï¸\")\n",
    "            continue\n",
    "        if len(threat_subset) > max_per_threat:\n",
    "            threat_sample = threat_subset.sample(n=max_per_threat, random_state=42)\n",
    "        else:\n",
    "            threat_sample = threat_subset\n",
    "        sampled_threats.append(threat_sample)\n",
    "\n",
    "    # Combining all\n",
    "    \n",
    "    modeling_data = pd.concat([benign_data] + sampled_threats, ignore_index=True)\n",
    "\n",
    "    print(f\"âœ… Reduced dataset contains: {len(modeling_data)} observations across {len(sampled_threats)} threats. âœ…\")\n",
    "    \n",
    "    return modeling_data.sample(frac=1.0, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfcf0a7-39ab-4e5b-bdf9-bba76676fa5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "modeling_dataset = reduce_dataset(data=combined_threats_dataset, label_column=\"Label\", threats=list_of_modeling_threats, max_per_threat=2_000_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc3ff90-3037-488f-99e0-c99b420b2530",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"âœ… Reduced dataset shape: {modeling_dataset.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d33fe6d-695b-4080-b24d-af8f338b029f",
   "metadata": {},
   "source": [
    "==============================  Dropping Unnecessary columns  =============================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986fefa6-e611-45f0-9521-ff428b5fc94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to be dropped\n",
    "\n",
    "columns_to_drop = ['Unnamed: 0', 'Flow ID', 'Source IP', 'Destination IP', \n",
    "                   'Timestamp', 'Fwd Header Length.1', 'SimillarHTTP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8dd685-1f74-401e-b371-b52f7516583e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_columns(data: pd.DataFrame, columns_to_drop: list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Drops specified columns from the dataset.\n",
    "\n",
    "    Arguments:\n",
    "        data (pd.DataFrame): Input dataset.\n",
    "        columns_to_drop (list): List of column names to be removed.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Cleaned dataset with specified columns dropped.\n",
    "    \"\"\"\n",
    "    if not isinstance(columns_to_drop, list):\n",
    "        raise TypeError(\"columns_to_drop must be a list of column names.\")\n",
    "\n",
    "    # Identifying only columns that exist in the DataFrame\n",
    "    \n",
    "    columns_present = [col for col in columns_to_drop if col in data.columns]\n",
    "\n",
    "    # Dropping only the columns that are present\n",
    "    \n",
    "    cleaned_data = data.drop(columns=columns_present)\n",
    "    \n",
    "    print(f\"âœ… Handled âœ…\")\n",
    "    \n",
    "    return cleaned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e83539-c85f-4ba4-a2d2-1ab838f5ec09",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_combined_threats_dataset = drop_columns(modeling_dataset, columns_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c058be61-970a-4d42-bf2e-5398b9b2ac4b",
   "metadata": {},
   "source": [
    "=================================== Shuffling the data  ==================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0335897-098d-463f-817b-eefb6b32066c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_dataset_in_batches(dataset: pd.DataFrame, batch_size: int = 100_000, random_seed: int = 42) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Shuffles the DataFrame in memory-safe batches for consistent randomness and scalability.\n",
    "\n",
    "    Arguments:\n",
    "        dataset (pd.DataFrame): Input dataset to shuffle.\n",
    "        batch_size (int): Size of each chunk to shuffle independently.\n",
    "        random_seed (int): Seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Shuffled dataset.\n",
    "    \"\"\"\n",
    "    if dataset.empty:\n",
    "        raise ValueError(\"The input DataFrame is empty.\")\n",
    "\n",
    "    number_of_rows = len(dataset)\n",
    "    rng = np.random.default_rng(seed=random_seed)\n",
    "    shuffled_indices = rng.permutation(number_of_rows)\n",
    "\n",
    "    shuffled_chunks = [\n",
    "        dataset.iloc[shuffled_indices[start:end]]\n",
    "        for start in range(0, number_of_rows, batch_size)\n",
    "        for end in [min(start + batch_size, number_of_rows)]\n",
    "    ]\n",
    "\n",
    "    shuffled_dataframe = pd.concat(shuffled_chunks, ignore_index=True)\n",
    "    \n",
    "    print(f\"âœ… Handled âœ…\")\n",
    "\n",
    "    return shuffled_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae8247e-787a-4d43-a8e4-c926d7b3b3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_combined_threats_dataset = shuffle_dataset_in_batches(cleaned_combined_threats_dataset, batch_size=100_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496bb2be-9a85-48a3-8748-1fecc834abe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_combined_threats_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76412a51-a739-4fcc-bb50-a47e5e5b84a1",
   "metadata": {},
   "source": [
    "===============================  Encoding the \"Label\" feature  ==============================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c571fa-da24-4d59-b3ec-dbce396ee4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_labels_one_hot_and_save(dataset: pd.DataFrame, label_column: str = \"Label\", output_path: str = \"Encoded_modeling_dataset.csv\"):\n",
    "    \"\"\"\n",
    "    One-hot encodes the \"Label\" column,\n",
    "    combining it with the feature set, \n",
    "    and saves the whole dataset to a CSV file.\n",
    "\n",
    "    Arguments:\n",
    "        dataset (pd.DataFrame): DataFrame with features and \"Label\" column.\n",
    "        label_column (str): Name of the \"Label\" column.\n",
    "        output_path (str): Path to save the final encoded dataset.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The combined dataset with one-hot encoded labels.\n",
    "    \"\"\"\n",
    "    if label_column not in dataset.columns:\n",
    "        raise KeyError(f\"âŒ The column '{label_column}' not found in the dataset. âŒ\")\n",
    "\n",
    "    # Separating and encoding\n",
    "    features_dataframe = dataset.drop(columns=[label_column])\n",
    "    labels_dataframe = pd.get_dummies(dataset[label_column])\n",
    "\n",
    "    # Combining features and encoded labels\n",
    "    final_dataframe = pd.concat([features_dataframe, labels_dataframe], axis=1)\n",
    "\n",
    "    # Saving to CSV\n",
    "    final_dataframe.to_csv(output_path, index=False)\n",
    "    \n",
    "    print(f\"âœ… Encoded dataset saved to: {output_path}\")\n",
    "    \n",
    "    return final_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84659368-817b-4239-8144-9ec962909134",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_encoded_dataset = encode_labels_one_hot_and_save(shuffled_combined_threats_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32e021b-2bc1-4433-9f69-b6734d534c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the combined DataFrame into features and labels\n",
    "\n",
    "label_columns = [\"BENIGN\", \"DrDoS_DNS\", \"DrDoS_NTP\", \"Syn\", \"UDP\"]\n",
    "\n",
    "X_features = final_encoded_dataset.drop(columns=label_columns)\n",
    "y_labels = final_encoded_dataset[label_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f709d3-65a2-4f62-b953-98406fa5956a",
   "metadata": {},
   "source": [
    "==================================  Splitting the dataset  ==================================  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5ebb85-8aa3-407a-894c-982bdd4d8949",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_and_scale_dataset(features: pd.DataFrame, labels: pd.DataFrame, test_size: float = 0.2, \n",
    "                            val_size: float = 0.0, scale_features: bool = True, random_seed: int = 42):\n",
    "    \"\"\"\n",
    "    Splits features and labels into train, test, and validation sets and scales features.\n",
    "\n",
    "    Arguments:\n",
    "        features (pd.DataFrame): Feature matrix (X).\n",
    "        labels (pd.DataFrame): One-hot encoded labels (y).\n",
    "        test_size (float): Proportion of the data for testing (default: 0.2).\n",
    "        val_size (float): Proportion of training set to reserve for validation (default: 0.0).\n",
    "        scale_features (bool): Whether to apply StandardScaler normalization to features (default: True).\n",
    "        random_seed (int): Random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary with keys:\n",
    "            - 'X_train', 'X_test', ('X_val' if val_size > 0)\n",
    "            - 'y_train', 'y_test', ('y_val' if val_size > 0)\n",
    "            - 'scaler' (optional, only if scale_features is True)\n",
    "    \"\"\"\n",
    "    # Stratified splitting based on class distribution.\n",
    "    \n",
    "    stratify_labels = labels.values.argmax(axis=1)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=test_size,\n",
    "                                                        stratify=stratify_labels, random_state=random_seed)\n",
    "\n",
    "    if val_size > 0:\n",
    "        stratify_train = y_train.values.argmax(axis=1)\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=val_size,\n",
    "                                                          stratify=stratify_train, random_state=random_seed)\n",
    "\n",
    "    # Normalizing when requested.\n",
    "    \n",
    "    scaler = None\n",
    "    if scale_features:\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "        if val_size > 0:\n",
    "            X_val = scaler.transform(X_val)\n",
    "\n",
    "    # Preparing output.\n",
    "    data_splits = {\n",
    "        \"X_train\": X_train,\n",
    "        \"X_test\": X_test,\n",
    "        \"y_train\": y_train.values,\n",
    "        \"y_test\": y_test.values,\n",
    "    }\n",
    "\n",
    "    if val_size > 0:\n",
    "        data_splits[\"X_val\"] = X_val\n",
    "        data_splits[\"y_val\"] = y_val.values\n",
    "\n",
    "    if scale_features:\n",
    "        data_splits[\"scaler\"] = scaler\n",
    "\n",
    "    print(f\"âœ… Handled âœ…\")\n",
    "    \n",
    "    return data_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74fb38b-4308-44e8-b1ea-d2e330eae752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dataset: 20 percent of the original dataset.\n",
    "\n",
    "# Validation dataset: 10 percent of the original dataset.\n",
    "\n",
    "splitted_dataset = split_and_scale_dataset(features=X_features, labels=y_labels, test_size=0.2,\n",
    "                                           val_size=0.125, scale_features=True, random_seed=42)\n",
    "\n",
    "X_train = splitted_dataset[\"X_train\"]\n",
    "X_val = splitted_dataset[\"X_val\"]\n",
    "X_test = splitted_dataset[\"X_test\"]\n",
    "\n",
    "y_train = splitted_dataset[\"y_train\"]\n",
    "y_val = splitted_dataset[\"y_val\"]\n",
    "y_test = splitted_dataset[\"y_test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea71ddc-0251-4402-bcdb-94cda4822d88",
   "metadata": {},
   "source": [
    "===================================  Splitting Report  ==================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60040d28-c772-4f9c-b9fa-5331fbb1ef1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_split_statistics(y_train, y_val=None, y_test=None, label_names=None):\n",
    "    \"\"\"\n",
    "    Prints the number and percentage of each class in the dataset splits.\n",
    "\n",
    "    Arguments:\n",
    "        y_train (np.ndarray or pd.DataFrame): One-hot encoded or label vector for training.\n",
    "        y_val (np.ndarray or pd.DataFrame, optional): Same for validation.\n",
    "        y_test (np.ndarray or pd.DataFrame, optional): Same for testing.\n",
    "        label_names (list, optional): Custom list of class labels (column headers).\n",
    "    \"\"\"\n",
    "    def summarize(y, name):\n",
    "        print(f\"\\nðŸ“Š Class Distribution in {name.upper()} Set:\")\n",
    "        \n",
    "        if isinstance(y, pd.DataFrame):\n",
    "            y_array = y.values\n",
    "            columns = y.columns\n",
    "        elif isinstance(y, np.ndarray):\n",
    "            y_array = y\n",
    "            columns = label_names if label_names else [f\"Class_{i}\" for i in range(y.shape[1])]\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported label type. Provide a NumPy array or DataFrame.\")\n",
    "\n",
    "        class_counts = y_array.sum(axis=0)\n",
    "        total = class_counts.sum()\n",
    "\n",
    "        for label, count in zip(columns, class_counts):\n",
    "            percent = (count / total) * 100\n",
    "            print(f\" - {label}: {int(count)} ({percent:.2f}%)\")\n",
    "\n",
    "        print(f\"âž¡ï¸ Total samples: {int(total)}\")\n",
    "\n",
    "    summarize(y_train, \"train\")\n",
    "    if y_val is not None:\n",
    "        summarize(y_val, \"val\")\n",
    "    if y_test is not None:\n",
    "        summarize(y_test, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043945e4-3c7b-4e7b-9401-8ed85e39e64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_split_statistics(y_train=splitted_dataset[\"y_train\"], y_val=splitted_dataset[\"y_val\"], \n",
    "                        y_test=splitted_dataset[\"y_test\"], label_names=y_labels.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209d57a1-58a7-4579-ba51-4b38e6ea733a",
   "metadata": {},
   "source": [
    "=================================  Normalizing the dataset  ================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80a4dbb-62d1-4e21-a096-1138901f9507",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_features(X_train: pd.DataFrame, X_test: pd.DataFrame, \n",
    "                       X_val: Optional[pd.DataFrame] = None) -> Tuple[np.ndarray, np.ndarray, Optional[np.ndarray], StandardScaler]:\n",
    "    \"\"\"\n",
    "    Normalizes training, testing, and validation feature sets using StandardScaler.\n",
    "\n",
    "    Arguments:\n",
    "        X_train (pd.DataFrame): Training features.\n",
    "        X_test (pd.DataFrame): Testing features.\n",
    "        X_val (Optional[pd.DataFrame]): Validation features.\n",
    "\n",
    "    Returns:\n",
    "        Tuple containing:\n",
    "            - X_train_scaled (np.ndarray): Scaled training data.\n",
    "            - X_test_scaled (np.ndarray): Scaled test data.\n",
    "            - X_val_scaled (np.ndarray or None): Scaled validation data if provided.\n",
    "            - scaler (StandardScaler): The fitted scaler.\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Fitting only on training data\n",
    "    \n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    X_val_scaled = scaler.transform(X_val) if X_val is not None else None\n",
    "\n",
    "    print(f\"âœ… Normalization completed: Train shape {X_train_scaled.shape}, Test shape {X_test_scaled.shape}\" + \n",
    "          (f\", Val shape {X_val_scaled.shape}\" if X_val_scaled is not None else \"\"))\n",
    "\n",
    "    return X_train_scaled, X_test_scaled, X_val_scaled, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9ef07c-97cf-4755-ae41-2ba4106c7572",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled, X_test_scaled, X_val_scaled, scaler = normalize_features(X_train=pd.DataFrame(X_train), \n",
    "                                                                         X_test=pd.DataFrame(X_test),\n",
    "                                                                         X_val=pd.DataFrame(X_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d47f42f-17f2-493b-823a-027124c60f1e",
   "metadata": {},
   "source": [
    "==================================  Converting to tensor  =================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f9e771-1c10-4f16-a528-305c2500eb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_tensor(X: Union[np.ndarray, pd.DataFrame], y: Union[np.ndarray, pd.Series, pd.DataFrame],\n",
    "                      one_hot_labels: bool = True) -> Tuple[torch.FloatTensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Converts features and labels to PyTorch tensors.\n",
    "\n",
    "    Arguments:\n",
    "        X: Feature matrix (NumPy array or DataFrame).\n",
    "        y: Labels (one-hot encoded or class indices).\n",
    "        one_hot_labels: If True, returns FloatTensor labels (for BCEWithLogitsLoss).\n",
    "                        If False, returns LongTensor class indices (for CrossEntropyLoss).\n",
    "\n",
    "    Returns:\n",
    "        Tuple:\n",
    "            - X_tensor (torch.FloatTensor): Tensor of features.\n",
    "            - y_tensor (torch.FloatTensor or torch.LongTensor): Labels tensor.\n",
    "    \"\"\"\n",
    "    if isinstance(X, (pd.DataFrame, pd.Series)):\n",
    "        X = X.values\n",
    "    if isinstance(y, (pd.DataFrame, pd.Series)):\n",
    "        y = y.values\n",
    "\n",
    "    X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "\n",
    "    if one_hot_labels:\n",
    "        y_tensor = torch.tensor(y, dtype=torch.float32)\n",
    "    else:\n",
    "        y_tensor = torch.tensor(np.argmax(y, axis=1), dtype=torch.long)\n",
    "\n",
    "    print(f\"âœ… Converted to tensors | X: {X_tensor.shape}, y: {y_tensor.shape}\")\n",
    "    \n",
    "    return X_tensor, y_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badc59af-0b89-44b2-bc41-246841b8d064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "\n",
    "X_train_tensor, y_train_tensor = convert_to_tensor(X_train, y_train, one_hot_labels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6c99f0-e0d8-4520-bf48-589e8fb59546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "\n",
    "X_test_tensor, y_test_tensor = convert_to_tensor(X_test, y_test, one_hot_labels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ed8e96-6129-46bb-a6ad-6bdf0c59d8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation\n",
    "\n",
    "X_val_tensor, y_val_tensor = convert_to_tensor(X_val, y_val, one_hot_labels=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9bd977-b451-4b41-8e96-c22c363fb3d8",
   "metadata": {},
   "source": [
    "### ====================  Building The Models  ===================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d60a1be-3cdb-4aa9-8fb7-d875b7d1d5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap training, validation, and test tensors into TensorDataset objects \n",
    "# to enable efficient batching and iteration using DataLoader during training and evaluation.\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e770934f-7d7d-46a1-ab9b-b4149ef86b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders for training, validation, and testing.\n",
    "# - `batch_size=128` defines the number of samples per batch.\n",
    "# - `shuffle=True` is used for training to ensure data is randomly sampled each epoch,\n",
    "#   while `shuffle=False` is used for validation and testing to maintain consistent ordering.\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b49231-9169-44bd-991f-852915b38344",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping = {\n",
    "    0: \"BENIGN\",\n",
    "    1: \"DrDoS_DNS\",\n",
    "    2: \"DrDoS_NTP\",\n",
    "    3: \"Syn\",\n",
    "    5: \"UDP\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3be0d9-7be0-4d77-97be-5ed7e44b213f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dimension = X_train_tensor.shape[1]           \n",
    "number_of_classes = y_labels.shape[1] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03db745a-7964-4d44-8c89-ef3a0bf2d55c",
   "metadata": {},
   "source": [
    "#### =======================  Building Individual Models  ======================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c5e4af-6b32-43f0-a373-7bf51b668bb5",
   "metadata": {},
   "source": [
    "##### ==========================  Convolutional Neural Network - (CNN)  =========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f649d2-5258-4ac2-8c5b-31bdfd655eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_intrusion_detection_cnn(input_dimension: int, number_of_classes: int = None, return_extractor: bool = False) -> nn.Module:\n",
    "    \"\"\"\n",
    "    Builds a CNN-based model for intrusion detection.\n",
    "    \n",
    "    Arguments:\n",
    "        input_dimension (int): Number of input features.\n",
    "        number_of_classes (int, optional): Number of output classes.\n",
    "                                           If None and return_extractor=True, only the feature extractor is returned.\n",
    "        return_extractor (bool, optional): If True, returns only the feature extractor module without the classifier.\n",
    "\n",
    "    Returns:\n",
    "        nn.Module: CNN feature extractor if return_extractor=True, otherwise full CNN model.\n",
    "    \"\"\"\n",
    "\n",
    "    class CNNModel(nn.Module):\n",
    "        def __init__(self, input_dimension, number_of_classes=None):\n",
    "            super(CNNModel, self).__init__()\n",
    "            self.hidden_dim = 128\n",
    "\n",
    "            self.feature_extractor = nn.Sequential(\n",
    "                nn.Conv1d(1, 64, kernel_size=5, padding=2),\n",
    "                nn.BatchNorm1d(64),\n",
    "                nn.GELU(),\n",
    "                nn.Dropout(0.25),\n",
    "\n",
    "                nn.Conv1d(64, 128, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm1d(128),\n",
    "                nn.GELU(),\n",
    "                nn.Dropout(0.3),\n",
    "\n",
    "                nn.Conv1d(128, self.hidden_dim, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm1d(self.hidden_dim),\n",
    "                nn.GELU(),\n",
    "\n",
    "                nn.AdaptiveAvgPool1d(1)\n",
    "            )\n",
    "\n",
    "            if number_of_classes is not None:\n",
    "                self.classifier = nn.Sequential(\n",
    "                    nn.Flatten(),\n",
    "                    nn.Linear(self.hidden_dim, 256),\n",
    "                    nn.BatchNorm1d(256),\n",
    "                    nn.GELU(),\n",
    "                    nn.Dropout(0.5),\n",
    "\n",
    "                    nn.Linear(256, 128),\n",
    "                    nn.BatchNorm1d(128),\n",
    "                    nn.GELU(),\n",
    "                    nn.Dropout(0.3),\n",
    "\n",
    "                    nn.Linear(128, number_of_classes)\n",
    "                )\n",
    "            else:\n",
    "                self.classifier = None  \n",
    "\n",
    "        def forward(self, x, return_features=False):\n",
    "            x = x.unsqueeze(1)             \n",
    "            x = self.feature_extractor(x)\n",
    "            x = x.squeeze(-1)               \n",
    "\n",
    "            if return_features or self.classifier is None:\n",
    "                return x\n",
    "            return self.classifier(x)\n",
    "\n",
    "    model = CNNModel(input_dimension, number_of_classes)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    if return_extractor:\n",
    "        return model.feature_extractor.to(device)\n",
    "    else:\n",
    "        return model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24de706f-a45e-4987-b699-b626e529f92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model = build_intrusion_detection_cnn(input_dimension, number_of_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e6b202-dc0e-4b5b-ba7c-c8803782d1ca",
   "metadata": {},
   "source": [
    "##### ===========================  Long Short-Term Memory  - (LSTM)  ==========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c80f7a-6f6b-4ab3-9479-59d004887174",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_intrusion_detection_lstm(input_dimension: int, number_of_classes: int = None, return_extractor: bool = False) -> nn.Module:\n",
    "    \"\"\"\n",
    "    Builds a robust LSTM-based model for intrusion detection.\n",
    "    \n",
    "    Arguments:\n",
    "        input_dimension (int): Number of input features (sequence length).\n",
    "        number_of_classes (int, optional): Number of output classes. If None, returns feature extractor only.\n",
    "        return_extractor (bool, optional): If True, returns only the feature extractor module.\n",
    "\n",
    "    Returns:\n",
    "        nn.Module: LSTM feature extractor if return_extractor=True, otherwise full LSTM classifier.\n",
    "    \"\"\"\n",
    "\n",
    "    class LSTMModel(nn.Module):\n",
    "        def __init__(self, input_dimension, number_of_classes=None):\n",
    "            super(LSTMModel, self).__init__()\n",
    "\n",
    "            self.hidden_dim = 128\n",
    "            self.num_layers = 2\n",
    "            self.feature_dim = 128\n",
    "\n",
    "            self.lstm = nn.LSTM(\n",
    "                input_size=1,\n",
    "                hidden_size=self.hidden_dim,\n",
    "                num_layers=self.num_layers,\n",
    "                batch_first=True,\n",
    "                dropout=0.3\n",
    "            )\n",
    "\n",
    "            self.feature_proj = nn.Sequential(\n",
    "                nn.Linear(self.hidden_dim, self.feature_dim),\n",
    "                nn.BatchNorm1d(self.feature_dim),\n",
    "                nn.GELU(),\n",
    "                nn.Dropout(0.3)\n",
    "            )\n",
    "\n",
    "            if number_of_classes is not None:\n",
    "                self.classifier = nn.Sequential(\n",
    "                    nn.Linear(self.feature_dim, 256),\n",
    "                    nn.BatchNorm1d(256),\n",
    "                    nn.GELU(),\n",
    "                    nn.Dropout(0.5),\n",
    "\n",
    "                    nn.Linear(256, 128),\n",
    "                    nn.BatchNorm1d(128),\n",
    "                    nn.GELU(),\n",
    "                    nn.Dropout(0.3),\n",
    "\n",
    "                    nn.Linear(128, number_of_classes)\n",
    "                )\n",
    "            else:\n",
    "                self.classifier = None\n",
    "\n",
    "        def forward(self, x, return_features=False):\n",
    "            x = x.unsqueeze(-1)\n",
    "            lstm_out, _ = self.lstm(x)\n",
    "            last_hidden = lstm_out[:, -1, :]\n",
    "            features = self.feature_proj(last_hidden) \n",
    "\n",
    "            if return_features or self.classifier is None:\n",
    "                return features\n",
    "            return self.classifier(features)\n",
    "\n",
    "    model = LSTMModel(input_dimension, number_of_classes)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    if return_extractor:\n",
    "        return model.feature_proj.to(device)\n",
    "    else:\n",
    "        return model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f4e1dd-68c8-4485-8943-a154441f359d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = build_intrusion_detection_lstm(input_dimension, number_of_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c155de45-ad8a-47b9-a460-b66924d98393",
   "metadata": {},
   "source": [
    "##### ========================  Transformer Neurol Network  - (Transformer)  ========================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a393d344-0cc7-4c24-ad94-2037f28f97a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_intrusion_detection_transformer(input_dimension: int, number_of_classes: int = None, return_extractor: bool = False) -> nn.Module:\n",
    "    \"\"\"\n",
    "    Builds an enhanced Transformer model tailored for tabular intrusion detection.\n",
    "\n",
    "    Arguments:\n",
    "        input_dimension (int): Number of input features.\n",
    "        number_of_classes (int, optional): Number of output classes. If None, acts only as a feature extractor.\n",
    "        return_extractor (bool, optional): If True, returns only the feature extractor.\n",
    "\n",
    "    Returns:\n",
    "        nn.Module: Transformer feature extractor or full classification model based on arguments.\n",
    "    \"\"\"\n",
    "\n",
    "    class TransformerModel(nn.Module):\n",
    "        def __init__(self, input_dimension, number_of_classes=None):\n",
    "            super(TransformerModel, self).__init__()\n",
    "\n",
    "            self.embedding_dim = 128\n",
    "            self.num_heads = 4\n",
    "            self.num_layers = 2\n",
    "            self.dropout_rate = 0.3\n",
    "\n",
    "            self.embedding = nn.Linear(1, self.embedding_dim)\n",
    "            self.positional_encoding = None\n",
    "\n",
    "            encoder_layer = nn.TransformerEncoderLayer(\n",
    "                d_model=self.embedding_dim,\n",
    "                nhead=self.num_heads,\n",
    "                dim_feedforward=256,\n",
    "                dropout=self.dropout_rate,\n",
    "                activation='gelu',\n",
    "                batch_first=True\n",
    "            )\n",
    "            self.transformer_encoder = nn.TransformerEncoder(\n",
    "                encoder_layer,\n",
    "                num_layers=self.num_layers\n",
    "            )\n",
    "\n",
    "            if number_of_classes is not None:\n",
    "                self.classifier = nn.Sequential(\n",
    "                    nn.Linear(self.embedding_dim, 256),\n",
    "                    nn.BatchNorm1d(256),\n",
    "                    nn.GELU(),\n",
    "                    nn.Dropout(self.dropout_rate),\n",
    "\n",
    "                    nn.Linear(256, 128),\n",
    "                    nn.BatchNorm1d(128),\n",
    "                    nn.GELU(),\n",
    "                    nn.Dropout(self.dropout_rate),\n",
    "\n",
    "                    nn.Linear(128, number_of_classes)\n",
    "                )\n",
    "            else:\n",
    "                self.classifier = None\n",
    "\n",
    "        def forward(self, x, return_features=False):\n",
    "            x = x.unsqueeze(-1)\n",
    "            x = self.embedding(x)\n",
    "\n",
    "            if self.positional_encoding is None or self.positional_encoding.size(1) != x.size(1):\n",
    "                self.positional_encoding = nn.Parameter(\n",
    "                    torch.randn(1, x.size(1), self.embedding_dim).to(x.device),\n",
    "                    requires_grad=True\n",
    "                )\n",
    "\n",
    "            x = x + self.positional_encoding\n",
    "            x = self.transformer_encoder(x)\n",
    "            x = x.mean(dim=1)\n",
    "\n",
    "            if return_features or self.classifier is None:\n",
    "                return x\n",
    "            return self.classifier(x)\n",
    "\n",
    "    model = TransformerModel(input_dimension, number_of_classes)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    if return_extractor:\n",
    "        return model.embedding.to(device)\n",
    "    else:\n",
    "        return model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16ceff8-cdcd-4231-a8ed-08b1fad3430d",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_model = build_intrusion_detection_transformer(input_dimension, number_of_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d18d67-74f8-4c2b-9d37-53f2da996815",
   "metadata": {},
   "source": [
    "##### =============================  Graph Neural Network  - (GNN)  ============================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ab7588-837c-4d26-9f75-c9da6c7125ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_intrusion_detection_gnn(input_dimension: int, number_of_classes: int = None, return_extractor: bool = False) -> nn.Module:\n",
    "    \"\"\"\n",
    "    Builds a GNN-inspired model for tabular intrusion detection using multihead attention.\n",
    "    \n",
    "    Supports both feature extraction mode and full classification mode.\n",
    "    \n",
    "    Arguments:\n",
    "        input_dimension (int): Number of input features.\n",
    "        number_of_classes (int, optional): Number of output classes. If None, acts only as a feature extractor.\n",
    "        return_extractor (bool, optional): If True, returns only the feature extractor module.\n",
    "    \n",
    "    Returns:\n",
    "        nn.Module: A feature extractor or full classifier model depending on the configuration.\n",
    "    \"\"\"\n",
    "\n",
    "    class GNNModel(nn.Module):\n",
    "        def __init__(self, input_dimension, number_of_classes=None):\n",
    "            super(GNNModel, self).__init__()\n",
    "\n",
    "            self.hidden_dim = 128\n",
    "            self.output_dim = 128\n",
    "            self.dropout_rate = 0.3\n",
    "\n",
    "            self.feature_proj = nn.Sequential(\n",
    "                nn.Linear(input_dimension, self.hidden_dim),\n",
    "                nn.BatchNorm1d(self.hidden_dim),\n",
    "                nn.GELU(),\n",
    "                nn.Dropout(self.dropout_rate),\n",
    "\n",
    "                nn.Linear(self.hidden_dim, self.output_dim),\n",
    "                nn.BatchNorm1d(self.output_dim),\n",
    "                nn.GELU(),\n",
    "                nn.Dropout(self.dropout_rate)\n",
    "            )\n",
    "\n",
    "            self.attention = nn.MultiheadAttention(\n",
    "                embed_dim=self.output_dim,\n",
    "                num_heads=4,\n",
    "                batch_first=True\n",
    "            )\n",
    "\n",
    "            if number_of_classes is not None:\n",
    "                self.classifier = nn.Sequential(\n",
    "                    nn.Linear(self.output_dim, 256),\n",
    "                    nn.BatchNorm1d(256),\n",
    "                    nn.GELU(),\n",
    "                    nn.Dropout(0.5),\n",
    "\n",
    "                    nn.Linear(256, 128),\n",
    "                    nn.BatchNorm1d(128),\n",
    "                    nn.GELU(),\n",
    "                    nn.Dropout(0.3),\n",
    "\n",
    "                    nn.Linear(128, number_of_classes)\n",
    "                )\n",
    "            else:\n",
    "                self.classifier = None\n",
    "\n",
    "        def forward(self, x, return_features=False):\n",
    "            if len(x.shape) == 2:\n",
    "                x = self.feature_proj(x)     \n",
    "                x = x.unsqueeze(1)            \n",
    "            elif len(x.shape) == 3:\n",
    "                x = x.mean(dim=1)             \n",
    "                x = self.feature_proj(x).unsqueeze(1)\n",
    "\n",
    "            attn_output, _ = self.attention(x, x, x)\n",
    "            attn_output = attn_output.squeeze(1)\n",
    "\n",
    "            if return_features or self.classifier is None:\n",
    "                return attn_output\n",
    "            return self.classifier(attn_output)\n",
    "\n",
    "    model = GNNModel(input_dimension, number_of_classes)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    return model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb20f8e-4fe3-4523-ba79-b4e34804e5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gnn_model = build_intrusion_detection_gnn(input_dimension, number_of_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8c6672-3ecf-4156-9461-71f15ffc706b",
   "metadata": {},
   "source": [
    "#### ========================  Training Individual Models  ========================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dae1cc1-3b75-4a7a-90c5-02309bab0fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, num_epochs=50, learning_rate=1e-3,\n",
    "                weight_decay=1e-4, device=None, save_best_model_path=None, use_scheduler=True,):\n",
    "    \"\"\"\n",
    "    Trains a PyTorch model and evaluates on the validation set at the end of each epoch.\n",
    "\n",
    "    Arguments:\n",
    "        model (nn.Module): PyTorch neural network model.\n",
    "        train_loader (DataLoader): Training data loader.\n",
    "        val_loader (DataLoader): Validation data loader.\n",
    "        num_epochs (int): Number of training epochs.\n",
    "        learning_rate (float): Optimizer learning rate.\n",
    "        weight_decay (float): L2 regularization term.\n",
    "        device (torch.device or str): 'cuda' or 'cpu' device target.\n",
    "        save_best_model_path (str): Path to save the best-performing model.\n",
    "        use_scheduler (bool): Whether to apply learning rate decay on the plateau.\n",
    "\n",
    "    Returns:\n",
    "        model (nn.Module): Best validation accuracy model.\n",
    "    \"\"\"\n",
    "\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model.to(device)\n",
    "    print(f\"ðŸš€ Training started on device: {device}\")\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    scheduler = (\n",
    "        torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"max\", patience=3, verbose=True)\n",
    "        if use_scheduler else None\n",
    "    )\n",
    "\n",
    "    best_val_acc = 0.0\n",
    "    best_model_state = None\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_train_loss = 0\n",
    "        y_true_train, y_pred_train = [], []\n",
    "\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_train_loss += loss.item()\n",
    "            y_pred_train.extend(outputs.argmax(dim=1).detach().cpu().numpy())\n",
    "            y_true_train.extend(y_batch.detach().cpu().numpy())\n",
    "\n",
    "        train_acc = accuracy_score(y_true_train, y_pred_train)\n",
    "        train_loss_avg = epoch_train_loss / len(train_loader)\n",
    "\n",
    "        # ==============================  Validation  ============================== #\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        y_true_val, y_pred_val = [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for X_val, y_val in val_loader:\n",
    "                X_val, y_val = X_val.to(device), y_val.to(device)\n",
    "                outputs = model(X_val)\n",
    "                loss = criterion(outputs, y_val)\n",
    "                val_loss += loss.item()\n",
    "                y_pred_val.extend(outputs.argmax(dim=1).cpu().numpy())\n",
    "                y_true_val.extend(y_val.cpu().numpy())\n",
    "\n",
    "        val_acc = accuracy_score(y_true_val, y_pred_val)\n",
    "        val_f1 = f1_score(y_true_val, y_pred_val, average=\"weighted\")\n",
    "        val_loss_avg = val_loss / len(val_loader)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch [{epoch + 1}/{num_epochs}] \"\n",
    "            f\"Train Loss: {train_loss_avg:.4f} | Train Acc: {train_acc:.4f} \"\n",
    "            f\"| Val Loss: {val_loss_avg:.4f} | Val Acc: {val_acc:.4f} | Val F1: {val_f1:.4f}\"\n",
    "        )\n",
    "\n",
    "        # Scheduler step\n",
    "        \n",
    "        if scheduler:\n",
    "            scheduler.step(val_acc)\n",
    "\n",
    "        # Saving the best model\n",
    "        \n",
    "        if save_best_model_path and val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_model_state = model.state_dict()\n",
    "            torch.save(best_model_state, save_best_model_path)\n",
    "            print(f\"âœ… New best model saved at {save_best_model_path}\")\n",
    "\n",
    "    print(f\"\\nâœ… Training complete in {time.time() - start_time:.2f} seconds.\")\n",
    "    print(f\"ðŸ¥‡ Best Validation Accuracy: {best_val_acc:.4f}\")\n",
    "\n",
    "    if best_model_state:\n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d6ce29-ed1d-4b12-b01f-4fe6a6a7b35b",
   "metadata": {},
   "source": [
    "##### =======================  Training Convolutional Neural Network - (CNN)  ======================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73609648-f565-4f25-a158-41689ab2f2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_cnn_model = train_model(model=cnn_model, train_loader=train_loader, val_loader=val_loader, num_epochs=10,\n",
    "                                learning_rate=1e-3, weight_decay=1e-5, save_best_model_path=\"cnn_threats_detection_model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1e8928-4764-4c1c-bbc9-48b8b5834106",
   "metadata": {},
   "source": [
    "##### =========================  Training Long Short-Term Memory - (LSTM)  ========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878e8a75-cc48-436c-9068-01d716e8b67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_lstm_model = train_model(model=lstm_model, train_loader=train_loader, val_loader=val_loader, num_epochs=10,\n",
    "                                 learning_rate=1e-3, weight_decay=1e-5, save_best_model_path=\"lstm_threats_detection_model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f9204c-ceb4-4748-bc42-0b61ca251ad4",
   "metadata": {},
   "source": [
    "##### ======================  Training Transformer Neurol Network - (Transformer)  ======================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b150f7-a815-48a5-8c73-9e2073d2bf0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_transformer_model = train_model(model=transformer_model, train_loader=train_loader, val_loader=val_loader, num_epochs=10,\n",
    "                                  learning_rate=1e-3, weight_decay=1e-5, save_best_model_path=\"transformer_threats_detection_model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74371cfe-1b32-48e6-8e53-445db60b4d14",
   "metadata": {},
   "source": [
    "##### ==========================  Training Graph Neural Network - (GNN)  =========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516690de-2cff-402c-8932-fb36221280e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_gnn_model = train_model(model=gnn_model, train_loader=train_loader, val_loader=val_loader, num_epochs=10, \n",
    "                                learning_rate=1e-3, weight_decay=1e-5, save_best_model_path=\"gnn_threats_detection_model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ef4229-4e43-4db2-8c42-5e37793f50ad",
   "metadata": {},
   "source": [
    "#### =======================  Evaluating Individual Models  ======================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623144bd-72c7-4c17-a60d-f3070cb47aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader, model_name=\"Model\", device=None, class_names=None, verbose=True):\n",
    "    \"\"\"\n",
    "    Evaluates a trained PyTorch model and visualizes both raw and normalized confusion matrices.\n",
    "\n",
    "    Arguments:\n",
    "        model (nn.Module): Trained model to evaluate.\n",
    "        data_loader (DataLoader): Evaluation DataLoader.\n",
    "        model_name (str): Name of the model (e.g., 'CNN', 'LSTM') for labeling purposes.\n",
    "        device (torch.device or str): Evaluation device.\n",
    "        class_names (list): Optional list of class names.\n",
    "        verbose (bool): If True, prints classification report.\n",
    "\n",
    "    Returns:\n",
    "        dict: Evaluation metrics including confusion matrices.\n",
    "    \"\"\"\n",
    "\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in data_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(y_batch.cpu().numpy())\n",
    "\n",
    "    # Computing the evaluation metrics\n",
    "    \n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "    recall = recall_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\\nðŸ“Š Classification Report for {model_name} Model:\\n\")\n",
    "        print(classification_report(all_labels, all_preds, target_names=class_names if class_names else None))\n",
    "\n",
    "    # Raw and normalized confusion matrices\n",
    "    \n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
    "\n",
    "    if verbose:\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\n",
    "        sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "                    xticklabels=class_names if class_names else \"auto\",\n",
    "                    yticklabels=class_names if class_names else \"auto\", ax=axs[0])\n",
    "        axs[0].set_title(f\"{model_name} - Raw Confusion Matrix\")\n",
    "        axs[0].set_xlabel(\"Predicted Label\")\n",
    "        axs[0].set_ylabel(\"True Label\")\n",
    "\n",
    "        sns.heatmap(cm_normalized, annot=True, fmt=\".2f\", cmap=\"YlGnBu\",\n",
    "                    xticklabels=class_names if class_names else \"auto\",\n",
    "                    yticklabels=class_names if class_names else \"auto\", ax=axs[1])\n",
    "        axs[1].set_title(f\"{model_name} - Normalized Confusion Matrix\")\n",
    "        axs[1].set_xlabel(\"Predicted Label\")\n",
    "        axs[1].set_ylabel(\"True Label\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1_score\": f1,\n",
    "        \"confusion_matrix\": cm.tolist(),\n",
    "        \"normalized_confusion_matrix\": cm_normalized.tolist(),\n",
    "        \"model_name\": model_name\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73ae2f8-8c18-4525-9096-b778bd943fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = list(label_mapping.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452eabc8-7f02-415f-9b7a-8ac7e00627e9",
   "metadata": {},
   "source": [
    "##### =======================  Evaluating Convolutional Neural Network - (CNN)  ======================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ba80bc-7280-4dfc-9cac-da34b13ca893",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_metrics = evaluate_model(model=trained_cnn_model, data_loader=test_loader, model_name=\"CNN\",\n",
    "                         class_names=class_names, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fdf816-8238-4f30-a4aa-27ebd57e9142",
   "metadata": {},
   "source": [
    "##### ========================  Evaluating Long Short-Term Memory - (LSTM)  ========================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87888099-b0e4-46f3-bcd2-749a764eafb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_metrics = evaluate_model(model=trained_lstm_model, data_loader=test_loader, model_name=\"LSTM\",\n",
    "                         class_names=class_names, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad1e2dd-6476-4258-bc90-cea32ec0d7ad",
   "metadata": {},
   "source": [
    "##### =====================  Evaluating Transformer Neurol Network - (Transformer)  ====================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7f0413-3bc4-4c4c-9c64-50e5f35e85de",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_metrics = evaluate_model(model=trained_transformer_model, data_loader=test_loader, model_name=\"Transformer\",\n",
    "                         class_names=class_names, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90839f01-8bca-43a7-b1ec-a2322dfe9204",
   "metadata": {},
   "source": [
    "##### =========================  Evaluating Graph Neural Network - (GNN)  ========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad9c477-ca76-4fe4-9c20-671f0c5c23dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "gnn_metrics = evaluate_model(model=trained_gnn_model, data_loader=test_loader, model_name=\"GNN\",\n",
    "                         class_names=class_names, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5fc43b-4b67-4f99-8654-7c5924e6ef49",
   "metadata": {},
   "source": [
    "#### ======================== Saving the Models Metrics  ========================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5cdbaa-73a0-41a7-8a10-638c1b6574b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_standalone_metrics = {\n",
    "    \"CNN\": cnn_metrics,\n",
    "    \"LSTM\": lstm_metrics,\n",
    "    \"Transformer\": transformer_metrics,\n",
    "    \"GNN\": gnn_metrics\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b21c1ba-14c4-4f66-9d83-57080115582e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_best_standalone_metrics_to_csv(metrics_dict: dict, save_path: str):\n",
    "    \n",
    "    dataframe = pd.DataFrame([{\n",
    "        \"Model\": model_name,\n",
    "        \"Accuracy\": metrics['accuracy'],\n",
    "        \"Precision\": metrics['precision'],\n",
    "        \"Recall\": metrics['recall'],\n",
    "        \"F1 Score\": metrics['f1_score']\n",
    "    } for model_name, metrics in metrics_dict.items()])\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    base_path = os.path.splitext(save_path)[0]\n",
    "    excel_path = f\"{base_path}_{timestamp}.xlsx\"\n",
    "    csv_path = f\"{base_path}_{timestamp}.csv\"\n",
    "\n",
    "    dataframe.to_excel(excel_path, index=False)\n",
    "    dataframe.to_csv(csv_path, index=False)\n",
    "    \n",
    "    print(f\"ðŸ“Š Best standalone model metrics exported to:\\n- Excel: {excel_path}\\n- CSV: {csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef2fb63-4d63-4820-8881-72a5238cee66",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_best_standalone_metrics_to_csv(best_standalone_metrics, \"best_standalone_models_metrics.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290672df-c6c9-4d93-a42e-9f887ad195a0",
   "metadata": {},
   "source": [
    "#### ======================  Building the Cascading Model  ======================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3367b0c-4b08-4c4d-a43b-c1ddb1280f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_intrusion_cascading_model(model_sequence: list, input_dimension: int, feature_dimension: int,\n",
    "                                    number_of_classes: int, freeze_extractors: bool = True, verbose: bool = False) -> nn.Module:\n",
    "    \"\"\"\n",
    "    Constructs a clean cascading model composed of four sequential deep learning models.\n",
    "    \n",
    "    Each model extracts and refines features from the previous one.\n",
    "    The final model directly performs classification.\n",
    "    \n",
    "    Returns:\n",
    "        nn.Module: A ready-to-train cascading model.\n",
    "    \"\"\"\n",
    "\n",
    "    class CascadingModel(nn.Module):\n",
    "        def __init__(self, models, feature_dimension, freeze_extractors, verbose):\n",
    "            super(CascadingModel, self).__init__()\n",
    "            assert len(models) == 4, \"Exactly four models must be provided.\"\n",
    "\n",
    "            self.model1, self.model2, self.model3, self.model4 = models\n",
    "            self.feature_dim = feature_dimension\n",
    "            self.verbose = verbose\n",
    "\n",
    "            if freeze_extractors:\n",
    "                for model in [self.model1, self.model2, self.model3]:\n",
    "                    for param in model.parameters():\n",
    "                        param.requires_grad = False\n",
    "\n",
    "        def forward(self, x):\n",
    "            device = next(self.parameters()).device\n",
    "            x = x.to(device)\n",
    "\n",
    "            x = self._pass_model(self.model1, x, \"Model 1\")\n",
    "            x = self._pass_model(self.model2, x, \"Model 2\")\n",
    "            x = self._pass_model(self.model3, x, \"Model 3\")\n",
    "\n",
    "            # Final model directly performs classification\n",
    "            out = self.model4(x, return_features=False)\n",
    "            return out\n",
    "\n",
    "        def _pass_model(self, model, x, model_name=\"Model\"):\n",
    "            if hasattr(model, \"forward\"):\n",
    "                try:\n",
    "                    x = model(x, return_features=True)\n",
    "                except TypeError as e:\n",
    "                    raise TypeError(f\"{model_name} must implement 'forward(x, return_features=True)': {e}\")\n",
    "            else:\n",
    "                raise AttributeError(f\"{model_name} does not implement a forward method.\")\n",
    "\n",
    "            # Enforcing feature dimension consistency\n",
    "            if x.shape[1] != self.feature_dim:\n",
    "                raise ValueError(\n",
    "                    f\"{model_name} output shape mismatch: expected ({x.shape[0]}, {self.feature_dim}), got {x.shape}\"\n",
    "                )\n",
    "            return x\n",
    "\n",
    "    model = CascadingModel(models=model_sequence, feature_dimension=feature_dimension, \n",
    "                           freeze_extractors=freeze_extractors, verbose=verbose)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    return model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1405975-b956-440c-93a5-a08eaba168f0",
   "metadata": {},
   "source": [
    "==============================  Loading the feature extractor  =============================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637ba5e2-38ec-4631-8ef4-10e84c5017f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_save_feature_extractors(input_dimension: int, number_of_classes: int, save_dir: str = \".\"):\n",
    "    \"\"\"\n",
    "    Loads trained full models, extracts feature layers, saves the extractors, \n",
    "    and returns them as a list.\n",
    "    \n",
    "    Args:\n",
    "        input_dimension (int): Number of input features.\n",
    "        number_of_classes (int): Number of output classes.\n",
    "        save_dir (str): Directory containing trained models and where to save feature extractors.\n",
    "    \n",
    "    Returns:\n",
    "        list[nn.Module]: Extracted feature extractors.\n",
    "    \"\"\"\n",
    "\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    ordered_model_names = [\"CNN\", \"LSTM\", \"Transformer\", \"GNN\"]\n",
    "\n",
    "    models_information = {\n",
    "        \"CNN\": {\n",
    "            \"builder\": build_intrusion_detection_cnn,\n",
    "            \"path\": os.path.join(save_dir, \"cnn_threats_detection_model.pt\"),\n",
    "            \"extractor_save\": os.path.join(save_dir, \"cnn_model_feature_extractor.pt\")\n",
    "        },\n",
    "        \"LSTM\": {\n",
    "            \"builder\": build_intrusion_detection_lstm,\n",
    "            \"path\": os.path.join(save_dir, \"lstm_threats_detection_model.pt\"),\n",
    "            \"extractor_save\": os.path.join(save_dir, \"lstm_model_feature_extractor.pt\")\n",
    "        },\n",
    "        \"Transformer\": {\n",
    "            \"builder\": build_intrusion_detection_transformer,\n",
    "            \"path\": os.path.join(save_dir, \"transformer_threats_detection_model.pt\"),\n",
    "            \"extractor_save\": os.path.join(save_dir, \"transformer_model_feature_extractor.pt\")\n",
    "        },\n",
    "        \"GNN\": {\n",
    "            \"builder\": build_intrusion_detection_gnn,\n",
    "            \"path\": os.path.join(save_dir, \"gnn_threats_detection_model.pt\"),\n",
    "            \"extractor_save\": os.path.join(save_dir, \"gnn_model_feature_extractor.pt\")\n",
    "        }\n",
    "    }\n",
    "\n",
    "    extractors = []\n",
    "\n",
    "    for model_name in ordered_model_names:\n",
    "        information = models_information[model_name]\n",
    "        print(f\" Processing {model_name.upper()}...\")\n",
    "\n",
    "        # Loading trained full models\n",
    "        \n",
    "        full_trained_models = information[\"builder\"](input_dimension, number_of_classes)\n",
    "        full_trained_models.load_state_dict(torch.load(information[\"path\"]), strict=False)\n",
    "\n",
    "        # Building and loading the feature extractors\n",
    "        \n",
    "        feature_extractor = information[\"builder\"](input_dimension, return_extractor=True)\n",
    "        feature_extractor.load_state_dict(full_trained_models.state_dict(), strict=False)\n",
    "\n",
    "        # Saving and appending\n",
    "        \n",
    "        torch.save(feature_extractor.state_dict(), information[\"extractor_save\"])\n",
    "        extractors.append(feature_extractor)\n",
    "\n",
    "        print(f\"âœ… Saved {model_name.upper()} feature extractor to {information['extractor_save']}\")\n",
    "\n",
    "    return extractors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06a220b-a84c-4313-b42f-4a97193c0a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the feature extractors: CNN, LSTM, Transformer, and GNN for the cascading models building.\n",
    "\n",
    "feature_extractor_models = extract_and_save_feature_extractors(input_dimension, number_of_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95aa8423-e031-4452-a5ae-2a7c2c6ab575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the cascading model\n",
    "\n",
    "cascading_model = build_intrusion_cascading_model(model_sequence=feature_extractor_models, input_dimension=input_dimension,\n",
    "                                                  feature_dimension=128, number_of_classes=number_of_classes,\n",
    "                                                  freeze_extractors=False, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fb76e8-1c81-4d63-ab91-fb68e8266cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinalClassifierWrapper(nn.Module):\n",
    "    def __init__(self, feature_model: nn.Module, feature_dimension: int, number_of_classes: int):\n",
    "        \"\"\"\n",
    "        Wraps a feature model and appends a final classification head.\n",
    "\n",
    "        Arguments:\n",
    "            feature_model (nn.Module): The feature extractor module (must support return_features=True).\n",
    "            feature_dimension (int): The expected dimension of extracted features.\n",
    "            number_of_classes (int): The number of target output classes.\n",
    "        \"\"\"\n",
    "        super(FinalClassifierWrapper, self).__init__()\n",
    "        self.feature_model = feature_model\n",
    "        self.feature_dim = feature_dimension\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.BatchNorm1d(feature_dimension),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(feature_dimension, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, number_of_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, return_features: bool = False):\n",
    "        features = self.feature_model(x, return_features=True)\n",
    "        \n",
    "        if features.shape[1] != self.feature_dim:\n",
    "            raise ValueError(f\"Expected feature dim {self.feature_dim}, but got {features.shape}\")\n",
    "            \n",
    "        if return_features:\n",
    "            return features\n",
    "            \n",
    "        return self.classifier(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed8b748-b032-4e2f-874f-597f843d9ff5",
   "metadata": {},
   "source": [
    "============================  Generating the cascading sequences  ============================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d060be1c-64c7-42c5-b815-65e1e4c824fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model_sequence(perm, model_builders, input_dimension, feature_dimension, number_of_classes):\n",
    "    \"\"\"\n",
    "    Generates a sequence of models for the cascading architecture.\n",
    "\n",
    "    Arguments:\n",
    "        perm (tuple): Ordered names of model types.\n",
    "        model_builders (dict): Dictionary mapping model names to builder functions.\n",
    "        input_dimension (int): Original input feature size.\n",
    "        feature_dimension (int): Unified feature size for cascading.\n",
    "        number_of_classes (int): Number of output classes.\n",
    "\n",
    "    Returns:\n",
    "        list[nn.Module]: List of 4 models, with the last wrapped in FinalClassifierWrapper.\n",
    "    \"\"\"\n",
    "    models = []\n",
    "    \n",
    "    for i, name in enumerate(perm):\n",
    "        if name not in model_builders:\n",
    "            raise ValueError(f\"Model '{name}' not found in model_builders.\")\n",
    "        \n",
    "        model_input_dimension = input_dimension if i == 0 else feature_dimension\n",
    "        model_output_dimension = feature_dimension\n",
    "\n",
    "        model = model_builders[name](model_input_dimension, model_output_dimension)\n",
    "        models.append(model)\n",
    "\n",
    "    # Wrapping the last model in a classification head\n",
    "    \n",
    "    final_model = FinalClassifierWrapper(models[-1], feature_dimension, number_of_classes)\n",
    "    models[-1] = final_model\n",
    "\n",
    "    return models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a13d3e-c784-4a98-9f38-0c187bcb6035",
   "metadata": {},
   "source": [
    "#### =======================  Training the Cascading Models  ======================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61e18e3-572e-4fa1-810e-d3e0f79f8b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cascading_model(model, train_loader, val_loader, num_epochs=50, learning_rate=1e-3,\n",
    "                weight_decay=1e-4, device=None, save_best_model_path=None, use_scheduler=True,):\n",
    "    \"\"\"\n",
    "    Trains the model and evaluates on the validation set at the end of each epoch.\n",
    "\n",
    "    Arguments:\n",
    "        model (nn.Module): PyTorch neural network model.\n",
    "        train_loader (DataLoader): Training data loader.\n",
    "        val_loader (DataLoader): Validation data loader.\n",
    "        num_epochs (int): Number of training epochs.\n",
    "        learning_rate (float): Optimizer learning rate.\n",
    "        weight_decay (float): L2 regularization term.\n",
    "        device (torch.device or str): 'cuda' or 'cpu' device target.\n",
    "        save_best_model_path (str): Path to save the best-performing model.\n",
    "        use_scheduler (bool): Whether to apply learning rate decay on the plateau.\n",
    "\n",
    "    Returns:\n",
    "        model (nn.Module): Best validation accuracy model.\n",
    "    \"\"\"\n",
    "\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model.to(device)\n",
    "    print(f\"ðŸš€ Training started on device: {device}\")\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    scheduler = (\n",
    "        torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"max\", patience=3, verbose=True)\n",
    "        if use_scheduler else None\n",
    "    )\n",
    "\n",
    "    best_val_acc = 0.0\n",
    "    best_model_state = None\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_train_loss = 0\n",
    "        y_true_train, y_pred_train = [], []\n",
    "\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_train_loss += loss.item()\n",
    "            y_pred_train.extend(outputs.argmax(dim=1).detach().cpu().numpy())\n",
    "            y_true_train.extend(y_batch.detach().cpu().numpy())\n",
    "\n",
    "        train_acc = accuracy_score(y_true_train, y_pred_train)\n",
    "        train_loss_avg = epoch_train_loss / len(train_loader)\n",
    "\n",
    "        # ==============================  Validation  ============================== #\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        y_true_val, y_pred_val = [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for X_val, y_val in val_loader:\n",
    "                X_val, y_val = X_val.to(device), y_val.to(device)\n",
    "                outputs = model(X_val)\n",
    "                loss = criterion(outputs, y_val)\n",
    "                val_loss += loss.item()\n",
    "                y_pred_val.extend(outputs.argmax(dim=1).cpu().numpy())\n",
    "                y_true_val.extend(y_val.cpu().numpy())\n",
    "\n",
    "        val_acc = accuracy_score(y_true_val, y_pred_val)\n",
    "        val_f1 = f1_score(y_true_val, y_pred_val, average=\"weighted\")\n",
    "        val_loss_avg = val_loss / len(val_loader)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch [{epoch + 1}/{num_epochs}] \"\n",
    "            f\"Train Loss: {train_loss_avg:.4f} | Train Acc: {train_acc:.4f} \"\n",
    "            f\"| Val Loss: {val_loss_avg:.4f} | Val Acc: {val_acc:.4f} | Val F1: {val_f1:.4f}\"\n",
    "        )\n",
    "\n",
    "        # Scheduler step\n",
    "        \n",
    "        if scheduler:\n",
    "            scheduler.step(val_acc)\n",
    "\n",
    "        # Saving the best model\n",
    "        \n",
    "        if save_best_model_path and val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_model_state = model.state_dict()\n",
    "            torch.save(best_model_state, save_best_model_path)\n",
    "            print(f\"âœ… New best model saved at {save_best_model_path}\")\n",
    "\n",
    "    print(f\"\\nâœ… Training complete in {time.time() - start_time:.2f} seconds.\")\n",
    "    print(f\"ðŸ¥‡ Best Validation Accuracy: {best_val_acc:.4f}\")\n",
    "\n",
    "    if best_model_state:\n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aeb711f-3c74-4b4e-9f84-bfeb225d34b7",
   "metadata": {},
   "source": [
    "#### ======================  Evaluating the Cascading Models  ====================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca15c17-0392-4365-acfa-213d5181c77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_cascading_model(model, data_loader, model_name=\"Model\", device=None, class_names=None, verbose=True):\n",
    "    \"\"\"\n",
    "    Evaluates the trained model and visualizes both raw and normalized confusion matrices.\n",
    "\n",
    "    Arguments:\n",
    "        model (nn.Module): Trained model to evaluate.\n",
    "        data_loader (DataLoader): Evaluation DataLoader.\n",
    "        model_name (str): Name of the model (e.g., 'CNN', 'LSTM') for labeling purposes.\n",
    "        device (torch.device or str): Evaluation device.\n",
    "        class_names (list): Optional list of class names.\n",
    "        verbose (bool): If True, prints classification report.\n",
    "\n",
    "    Returns:\n",
    "        dict: Evaluation metrics including confusion matrices.\n",
    "    \"\"\"\n",
    "\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in data_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(y_batch.cpu().numpy())\n",
    "\n",
    "    # Computing the evaluation metrics\n",
    "    \n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "    recall = recall_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\\nðŸ“Š Classification Report for {model_name} Model:\\n\")\n",
    "        print(classification_report(all_labels, all_preds, target_names=class_names if class_names else None))\n",
    "\n",
    "    # Raw and normalized confusion matrices\n",
    "    \n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
    "\n",
    "    if verbose:\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\n",
    "        sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "                    xticklabels=class_names if class_names else \"auto\",\n",
    "                    yticklabels=class_names if class_names else \"auto\", ax=axs[0])\n",
    "        axs[0].set_title(f\"{model_name} - Raw Confusion Matrix\")\n",
    "        axs[0].set_xlabel(\"Predicted Label\")\n",
    "        axs[0].set_ylabel(\"True Label\")\n",
    "\n",
    "        sns.heatmap(cm_normalized, annot=True, fmt=\".2f\", cmap=\"YlGnBu\",\n",
    "                    xticklabels=class_names if class_names else \"auto\",\n",
    "                    yticklabels=class_names if class_names else \"auto\", ax=axs[1])\n",
    "        axs[1].set_title(f\"{model_name} - Normalized Confusion Matrix\")\n",
    "        axs[1].set_xlabel(\"Predicted Label\")\n",
    "        axs[1].set_ylabel(\"True Label\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1_score\": f1,\n",
    "        \"confusion_matrix\": cm.tolist(),\n",
    "        \"normalized_confusion_matrix\": cm_normalized.tolist(),\n",
    "        \"model_name\": model_name\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0684243-fea8-4ba2-8482-21e0c7b5cf5a",
   "metadata": {},
   "source": [
    "#### ====================  Saving the Cascading Model Summary  ===================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c64054-cd75-4557-996d-c486b85a32ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_builders = {\n",
    "    \"CNN\": build_intrusion_detection_cnn,\n",
    "    \"LSTM\": build_intrusion_detection_lstm,\n",
    "    \"Transformer\": build_intrusion_detection_transformer,\n",
    "    \"GNN\": build_intrusion_detection_gnn\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cb1b6c-0568-4ab6-9c32-57ce33c4a8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(model, dataloader, device=None, class_names=None, normalize=True, title=\"Confusion Matrix\", cmap=\"Blues\"):\n",
    "    \"\"\"\n",
    "    Plots the confusion matrix of a trained model on a given dataset.\n",
    "\n",
    "    Arguments:\n",
    "        model (nn.Module): Trained model to evaluate.\n",
    "        dataloader (DataLoader): DataLoader with samples to evaluate.\n",
    "        device (torch.device, optional): Computation device. If None, auto-selects.\n",
    "        class_names (list, optional): List of class labels to show on axes.\n",
    "        normalize (bool): If True, normalize values by row (per class).\n",
    "        title (str): Title of the plot.\n",
    "        cmap (str): Matplotlib colormap to use.\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in dataloader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(y_batch.numpy())\n",
    "\n",
    "    # Generating the confusion matrix\n",
    "    \n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
    "\n",
    "    if class_names is None:\n",
    "        unique = sorted(set(all_labels + all_preds))\n",
    "        class_names = [str(c) for c in unique]\n",
    "\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
    "    disp.plot(cmap=cmap, values_format=\".2f\" if normalize else \"d\")\n",
    "    plt.title(title)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29eeb6eb-a829-4050-8f3c-ed2e48a44f6d",
   "metadata": {},
   "source": [
    "==============================  Cascading Models Benchmark  ========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b9a554-0879-4f1f-84d0-40c8daa288d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_all_permutations_model(model_builders: dict, input_dimension: int, feature_dimension: int, number_of_classes: int,\n",
    "                                     train_loader, val_loader, test_loader, num_epochs: int = 50, \n",
    "                                     save_dir: str = \"./permutation_models\", verbose: bool = False):\n",
    "    \"\"\"\n",
    "    Benchmarks all 24 permutations of CNN, LSTM, Transformer, GNN models in a cascading architecture.\n",
    "\n",
    "    Trains, evaluates, and records F1 scores for each permutation. \n",
    "    Automatically plots confusion matrix for the best model and F1 score distribution by the leading model.\n",
    "    \"\"\"\n",
    "\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model_names = list(model_builders.keys())\n",
    "\n",
    "    best_f1 = -1.0\n",
    "    best_model_info = {}\n",
    "    results = []\n",
    "    grouped_results = {name: [] for name in model_names}\n",
    "\n",
    "    print(f\"\\nðŸ“Š Starting 24-model permutation benchmark...\\n\")\n",
    "\n",
    "    for i, models in enumerate(permutations(model_names), 1):\n",
    "        print(f\"\\nðŸ” Permutation {i}/24: {models}\")\n",
    "\n",
    "        # Generating the model sequence\n",
    "        model_sequence = generate_model_sequence(perm=models, model_builders=model_builders, input_dimension=input_dimension,\n",
    "                                                 feature_dimension=feature_dimension, number_of_classes=number_of_classes)\n",
    "\n",
    "        # Building the full cascading model\n",
    "        cascading_model = build_intrusion_cascading_model(model_sequence=model_sequence, input_dimension=input_dimension,\n",
    "                                                          feature_dimension=feature_dimension, number_of_classes=number_of_classes, freeze_extractors=False, verbose=verbose)\n",
    "\n",
    "        model_name = \"_\".join(models)\n",
    "        model_path = os.path.join(save_dir, f\"{model_name}.pt\")\n",
    "\n",
    "        # Training the model\n",
    "        trained_cascading_model = train_cascading_model(model=cascading_model, train_loader=train_loader, val_loader=val_loader,\n",
    "                                    num_epochs=num_epochs, learning_rate=1e-3, weight_decay=1e-5, \n",
    "                                    save_best_model_path=model_path, use_scheduler=True, device=device)\n",
    "\n",
    "        # Loading best weights\n",
    "        trained_cascading_model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "        # Evaluating the model\n",
    "        metrics = evaluate_cascading_model(model=trained_cascading_model, data_loader=test_loader, model_name=model_name, \n",
    "                                 device=device, class_names=[str(i) for i in range(number_of_classes)], verbose=False)\n",
    "\n",
    "        print(f\"âœ… Permutation {i} completed. Test F1: {metrics['f1_score']:.4f}\")\n",
    "\n",
    "        results.append({\n",
    "            \"permutation\": models,\n",
    "            \"metrics\": metrics,\n",
    "            \"model_path\": model_path\n",
    "        })\n",
    "\n",
    "        grouped_results[models[0]].append(metrics['f1_score'])\n",
    "\n",
    "        if metrics['f1_score'] > best_f1:\n",
    "            best_f1 = metrics['f1_score']\n",
    "            best_model_info = {\n",
    "                \"permutation\": models,\n",
    "                \"metrics\": metrics,\n",
    "                \"model_path\": model_path\n",
    "            }\n",
    "\n",
    "    # Saving all results\n",
    "    summary_path = os.path.join(save_dir, \"benchmark_results.json\")\n",
    "    with open(summary_path, \"w\") as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "\n",
    "    print(f\"\\nðŸ† Best permutation: {best_model_info['permutation']} | F1-score: {best_model_info['metrics']['f1_score']:.4f}\")\n",
    "    print(f\"ðŸ“ Saved best model at: {best_model_info['model_path']}\")\n",
    "    print(f\"ðŸ—‚ Benchmark results saved to: {summary_path}\")\n",
    "\n",
    "    # Rebuilding and reloading the best model\n",
    "    best_model_sequence = generate_model_sequence(perm=best_model_info['permutation'], model_builders=model_builders,\n",
    "                                                  input_dimension=input_dimension, feature_dimension=feature_dimension, number_of_classes=number_of_classes)\n",
    "\n",
    "    best_model = build_intrusion_cascading_model(model_sequence=best_model_sequence, input_dimension=input_dimension, \n",
    "                                                 feature_dimension=feature_dimension, number_of_classes=number_of_classes, freeze_extractors=False, verbose=False)\n",
    "\n",
    "    best_model.load_state_dict(torch.load(best_model_info['model_path']))\n",
    "\n",
    "    # Plotting the confusion matrix for the best model\n",
    "    plot_confusion_matrix(model=best_model, dataloader=test_loader, device=device,\n",
    "                          class_names=[str(i) for i in range(number_of_classes)], title=\"Best Cascading Model Confusion Matrix\")\n",
    "\n",
    "    # Visualizing F1 distribution by leading model\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.boxplot(data=[grouped_results[k] for k in grouped_results.keys()],\n",
    "                palette=\"coolwarm\")\n",
    "    plt.xticks(ticks=range(len(grouped_results)), labels=grouped_results.keys())\n",
    "    plt.title(\"ðŸ“ˆ F1 Score Distribution by Leading Model\")\n",
    "    plt.ylabel(\"F1 Score\")\n",
    "    plt.xlabel(\"Leading Model\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return best_model_info, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef22c8dc-9d10-41f8-9cc2-030af25a0a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_information, full_results = benchmark_all_permutations_model(model_builders=model_builders, input_dimension=128,\n",
    "                                                      feature_dimension=128, number_of_classes=5, train_loader=train_loader,\n",
    "                                                      val_loader=val_loader, test_loader=test_loader, num_epochs=10, save_dir=\"./permutation_models\",\n",
    "                                                      verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65524e9-bf5c-49a8-9563-92d218cb53c5",
   "metadata": {},
   "source": [
    "========================  Saving the Cascading Permutations Models Metrics  ========================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63962e37-b9d1-4ce5-aaf2-84db17f051a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_cascading_metrics_to_csv(results: list, save_path: str):\n",
    "    \n",
    "    dataframe = pd.DataFrame([{\n",
    "        \"Permutation\": \" > \".join(result['permutation']),\n",
    "        \"Accuracy\": result['metrics']['accuracy'],\n",
    "        \"Precision\": result['metrics']['precision'],\n",
    "        \"Recall\": result['metrics']['recall'],\n",
    "        \"F1 Score\": result['metrics']['f1_score'],\n",
    "        \"Model Path\": result['model_path']\n",
    "    } for result in results])\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    base_path = os.path.splitext(save_path)[0]\n",
    "    excel_path = f\"{base_path}_{timestamp}.xlsx\"\n",
    "    csv_path = f\"{base_path}_{timestamp}.csv\"\n",
    "\n",
    "    dataframe.to_excel(excel_path, index=False)\n",
    "    dataframe.to_csv(csv_path, index=False)\n",
    "    \n",
    "    print(f\"ðŸ“Š Cascading benchmark results exported to:\\n- Excel: {excel_path}\\n- CSV: {csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b537ec67-b9a8-412a-a93d-9bb551d6fc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_cascading_metrics_to_csv(full_results, \"All_cascading_permutations_metrics_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04277906-7853-4fc8-bac2-bd77dad68c9b",
   "metadata": {},
   "source": [
    "========================= Building the Cascading models Manually  ========================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c35eef4-a839-416a-906f-affb38eb65c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40640d7b-c6a1-4a46-9842-fb2a833ac246",
   "metadata": {},
   "outputs": [],
   "source": [
    "cltg_model = build_intrusion_cascading_model(\n",
    "    model_sequence=generate_model_sequence(\n",
    "        perm=(\"CNN\", \"LSTM\", \"Transformer\", \"GNN\"),  \n",
    "        model_builders=model_builders,               \n",
    "        input_dimension=input_dimension,                          \n",
    "        feature_dimension=128,                       \n",
    "        number_of_classes=number_of_classes                        \n",
    "    ),\n",
    "    input_dimension=input_dimension,\n",
    "    feature_dimension=128,\n",
    "    number_of_classes=number_of_classes,\n",
    "    freeze_extractors=False,                         \n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38b1475-0933-4a95-b562-14e409563e11",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Training the cascading model\n",
    "\n",
    "trained_cltg_model = train_cascading_model(model=tlgc_model, train_loader=train_loader, val_loader=val_loader,\n",
    "                                    num_epochs=10, learning_rate=1e-3, weight_decay=1e-5, \n",
    "                                    save_best_model_path=\"./permutations/CNN_LSTM_Transformer_GNN.pt\", use_scheduler=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f33eb8-4e64-4c8a-aa2b-5ddb53d9223d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the cascading model\n",
    "\n",
    "cltg_metrics = evaluate_model(model=trained_cltg_model, data_loader=test_loader, model_name=\"CLTG\",\n",
    "                         class_names=class_names, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3d9ac2-38b2-4e61-857f-840788c1eca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "clgt_model = build_intrusion_cascading_model(\n",
    "    model_sequence=generate_model_sequence(\n",
    "        perm=(\"CNN\", \"LSTM\", \"GNN\", \"Transformer\"),\n",
    "        model_builders=model_builders,              \n",
    "        input_dimension=input_dimension,                       \n",
    "        feature_dimension=128,                      \n",
    "        number_of_classes=number_of_classes                      \n",
    "    ),\n",
    "    input_dimension=input_dimension,\n",
    "    feature_dimension=128,\n",
    "    number_of_classes=number_of_classes,\n",
    "    freeze_extractors=False,                         \n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3359bab-0d32-462a-9ef1-bf5cb3c522de",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Training the cascading model\n",
    "\n",
    "trained_clgt_model = train_cascading_model(model=clgt_model, train_loader=train_loader, val_loader=val_loader,\n",
    "                                    num_epochs=10, learning_rate=1e-3, weight_decay=1e-5, \n",
    "                                    save_best_model_path=\"./permutations/CNN_LSTM_GNN_Transformer.pt\", use_scheduler=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dea081f-f905-430b-b146-72e9610852cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the cascading model\n",
    "\n",
    "clgt_metrics = evaluate_model(model=trained_clgt_model, data_loader=test_loader, model_name=\"CLGT\",\n",
    "                         class_names=class_names, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb190694-023f-4616-9349-c494f7a88d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctlg_model = build_intrusion_cascading_model(\n",
    "    model_sequence=generate_model_sequence(\n",
    "        perm=(\"CNN\",\"Transformer\", \"LSTM\", \"GNN\"), \n",
    "        model_builders=model_builders,               \n",
    "        input_dimension=input_dimension,                         \n",
    "        feature_dimension=128,                       \n",
    "        number_of_classes=number_of_classes                 \n",
    "    ),\n",
    "    input_dimension=input_dimension,\n",
    "    feature_dimension=128,\n",
    "    number_of_classes=number_of_classes,\n",
    "    freeze_extractors=False,                        \n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe17ad3-3f45-4072-a40e-7e83fa6a47f4",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Training the cascading model\n",
    "\n",
    "trained_ctlg_model = train_cascading_model(model=ctlg_model, train_loader=train_loader, val_loader=val_loader,\n",
    "                                    num_epochs=10, learning_rate=1e-3, weight_decay=1e-5, \n",
    "                                    save_best_model_path=\"./permutations/CNN_Transformer_LSTM_GNN.pt\", use_scheduler=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32a69fb-b1fa-46d5-a41c-301b5e6868de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the cascading model\n",
    "\n",
    "ctlg_metrics = evaluate_model(model=trained_ctlg_model, data_loader=test_loader, model_name=\"CTLG\",\n",
    "                         class_names=class_names, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65b1c6f-ad21-44a2-8799-242014a872cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctgl_model = build_intrusion_cascading_model(\n",
    "    model_sequence=generate_model_sequence(\n",
    "        perm=(\"CNN\", \"Transformer\", \"GNN\", \"LSTM\"), \n",
    "        model_builders=model_builders,               \n",
    "        input_dimension=input_dimension,                          \n",
    "        feature_dimension=128,                       \n",
    "        number_of_classes=number_of_classes                         \n",
    "    ),\n",
    "    input_dimension=input_dimension,\n",
    "    feature_dimension=128,\n",
    "    number_of_classes=number_of_classes,\n",
    "    freeze_extractors=False,                        \n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de1c5bd-119b-488e-92a0-830c1322e135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the cascding model\n",
    "\n",
    "trained_ctgl_model = train_cascading_model(model=ctgl_model, train_loader=train_loader, val_loader=val_loader,\n",
    "                                    num_epochs=10, learning_rate=1e-3, weight_decay=1e-5, \n",
    "                                    save_best_model_path=\"./permutations/CNN_Transformer_GNN_LSTM.pt\", use_scheduler=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dada3821-73ed-420b-9356-424a8bb3e6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evalutating the cascading model\n",
    "\n",
    "ctgl_metrics = evaluate_model(model=trained_ctgl_model, data_loader=test_loader, model_name=\"CTGL\",\n",
    "                         class_names=class_names, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3d9864-a87f-40ac-8127-4e9c5481fb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cglt_model = build_intrusion_cascading_model(\n",
    "    model_sequence=generate_model_sequence(\n",
    "        perm=(\"CNN\", \"GNN\", \"LSTM\", \"Transformer\"), \n",
    "        model_builders=model_builders,               \n",
    "        input_dimension=input_dimension,                         \n",
    "        feature_dimension=128,                       \n",
    "        number_of_classes=number_of_classes                          \n",
    "    ),\n",
    "    input_dimension=input_dimension,\n",
    "    feature_dimension=128,\n",
    "    number_of_classes=number_of_classes,\n",
    "    freeze_extractors=False,                        \n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba03040-bb22-472b-93d9-273c40cfd814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the cascading model\n",
    "\n",
    "trained_cglt_model = train_cascading_model(model=cglt_model, train_loader=train_loader, val_loader=val_loader,\n",
    "                                    num_epochs=10, learning_rate=1e-3, weight_decay=1e-5, \n",
    "                                    save_best_model_path=\"./permutations/CNN_GNN_LSTM_Transformer.pt\", use_scheduler=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1707b2-26ad-497e-8640-38ef9bf081bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the cascading model\n",
    "\n",
    "cglt_metrics = evaluate_model(model=trained_cglt_model, data_loader=test_loader, model_name=\"CGLT\",\n",
    "                         class_names=class_names, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4940908-38bb-4fcf-9183-f7f53e27efd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cgtl_model = build_intrusion_cascading_model(\n",
    "    model_sequence=generate_model_sequence(\n",
    "        perm=(\"CNN\", \"GNN\", \"Transformer\", \"LSTM\"), \n",
    "        model_builders=model_builders,               \n",
    "        input_dimension=input_dimension,                         \n",
    "        feature_dimension=128,                       \n",
    "        number_of_classes=number_of_classes                          \n",
    "    ),\n",
    "    input_dimension=input_dimension,\n",
    "    feature_dimension=128,\n",
    "    number_of_classes=number_of_classes,\n",
    "    freeze_extractors=False,                        \n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352f95a8-dea6-4100-b60c-17c7966cd609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the cascading model\n",
    "\n",
    "trained_cgtl_model = train_cascading_model(model=cgtl_model, train_loader=train_loader, val_loader=val_loader,\n",
    "                                    num_epochs=10, learning_rate=1e-3, weight_decay=1e-5, \n",
    "                                    save_best_model_path=\"./permutations/CNN_GNN_Transformer_LSTM.pt\", use_scheduler=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1728505b-0b9d-4011-b99b-4fb2b26d8cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the cascading model\n",
    "\n",
    "cgtl_metrics = evaluate_model(model=trained_cgtl_model, data_loader=test_loader, model_name=\"CGTL\",\n",
    "                         class_names=class_names, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1677996a-d415-4879-aa19-551572929fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lctg_model = build_intrusion_cascading_model(\n",
    "    model_sequence=generate_model_sequence(\n",
    "        perm=(\"LSTM\", \"CNN\", \"Transformer\", \"GNN\"), \n",
    "        model_builders=model_builders,               \n",
    "        input_dimension=input_dimension,                         \n",
    "        feature_dimension=128,                       \n",
    "        number_of_classes=number_of_classes                          \n",
    "    ),\n",
    "    input_dimension=input_dimension,\n",
    "    feature_dimension=128,\n",
    "    number_of_classes=number_of_classes,\n",
    "    freeze_extractors=False,                        \n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5465adb-93da-48c6-bf2e-d8c291e77dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the cascading model\n",
    "\n",
    "trained_lctg_model = train_cascading_model(model=lctg_model, train_loader=train_loader, val_loader=val_loader,\n",
    "                                    num_epochs=10, learning_rate=1e-3, weight_decay=1e-5, \n",
    "                                    save_best_model_path=\"./permutations/LSTM_CNN_Transformer_GNN.pt\", use_scheduler=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c179610-c7fc-4dc5-9eac-df9a6b9ae927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the cascading model\n",
    "\n",
    "lctg_metrics = evaluate_model(model=trained_lctg_model, data_loader=test_loader, model_name=\"LCTG\",\n",
    "                         class_names=class_names, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04274982-17b0-49e1-838d-fc690fe0e03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lcgt_model = build_intrusion_cascading_model(\n",
    "    model_sequence=generate_model_sequence(\n",
    "        perm=(\"LSTM\", \"CNN\", \"GNN\", \"Transformer\"), \n",
    "        model_builders=model_builders,               \n",
    "        input_dimension=input_dimension,                         \n",
    "        feature_dimension=128,                       \n",
    "        number_of_classes=number_of_classes                          \n",
    "    ),\n",
    "    input_dimension=input_dimension,\n",
    "    feature_dimension=128,\n",
    "    number_of_classes=number_of_classes,\n",
    "    freeze_extractors=False,                        \n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9e7bec-028c-4e77-be47-c8e16ce383e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the cascading model\n",
    "\n",
    "trained_lcgt_model = train_cascading_model(model=lcgt_model, train_loader=train_loader, val_loader=val_loader,\n",
    "                                    num_epochs=10, learning_rate=1e-3, weight_decay=1e-5, \n",
    "                                    save_best_model_path=\"./permutations/LSTM_CNN_GNN_Transformer.pt\", use_scheduler=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29dcfef5-af07-4c99-ae4f-5799f0ae709f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the cascading model\n",
    "\n",
    "lcgt_metrics = evaluate_model(model=trained_lcgt_model, data_loader=test_loader, model_name=\"LCGT\",\n",
    "                         class_names=class_names, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142b88f8-62f7-4520-b0f6-262aac558b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ltcg_model = build_intrusion_cascading_model(\n",
    "    model_sequence=generate_model_sequence(\n",
    "        perm=(\"LSTM\", \"Transformer\", \"CNN\", \"GNN\"), \n",
    "        model_builders=model_builders,               \n",
    "        input_dimension=input_dimension,                         \n",
    "        feature_dimension=128,                       \n",
    "        number_of_classes=number_of_classes                          \n",
    "    ),\n",
    "    input_dimension=input_dimension,\n",
    "    feature_dimension=128,\n",
    "    number_of_classes=number_of_classes,\n",
    "    freeze_extractors=False,                        \n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce11ca04-19c2-458f-95ec-7868a506d079",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the cascading model\n",
    "\n",
    "trained_ltcg_model = train_cascading_model(model=ltcg_model, train_loader=train_loader, val_loader=val_loader,\n",
    "                                    num_epochs=10, learning_rate=1e-3, weight_decay=1e-5, \n",
    "                                    save_best_model_path=\"./permutations/LSTM_Transformer_CNN_GNN.pt\", use_scheduler=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227b26c0-b7cd-4cfe-880f-e18dc0bc6d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the cascading model\n",
    "\n",
    "ltcg_metrics = evaluate_model(model=trained_ltcg_model, data_loader=test_loader, model_name=\"LTCG\",\n",
    "                         class_names=class_names, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f1e2d9-e31c-4d2d-9a63-2e3a4cbf01e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ltgc_model = build_intrusion_cascading_model(\n",
    "    model_sequence=generate_model_sequence(\n",
    "        perm=(\"LSTM\", \"Transformer\", \"GNN\", \"CNN\"), \n",
    "        model_builders=model_builders,               \n",
    "        input_dimension=input_dimension,                         \n",
    "        feature_dimension=128,                       \n",
    "        number_of_classes=number_of_classes                          \n",
    "    ),\n",
    "    input_dimension=input_dimension,\n",
    "    feature_dimension=128,\n",
    "    number_of_classes=number_of_classes,\n",
    "    freeze_extractors=False,                        \n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c22ddb-4a08-43c3-83ac-9b202c9acae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the cascading model\n",
    "\n",
    "trained_ltgc_model = train_cascading_model(model=ltgc_model, train_loader=train_loader, val_loader=val_loader,\n",
    "                                    num_epochs=10, learning_rate=1e-3, weight_decay=1e-5, \n",
    "                                    save_best_model_path=\"./permutations/LSTM_Transformer_GNN_CNN.pt\", use_scheduler=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c669ba9-ea13-4c74-8792-7e94a058c783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the cascading model\n",
    "\n",
    "ltgc_metrics = evaluate_model(model=trained_ltgc_model, data_loader=test_loader, model_name=\"LTGC\",\n",
    "                         class_names=class_names, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b589a5-6681-49e9-91bf-b720dd099b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgct_model = build_intrusion_cascading_model(\n",
    "    model_sequence=generate_model_sequence(\n",
    "        perm=(\"LSTM\", \"GNN\", \"CNN\", \"Transformer\"), \n",
    "        model_builders=model_builders,               \n",
    "        input_dimension=input_dimension,                         \n",
    "        feature_dimension=128,                       \n",
    "        number_of_classes=number_of_classes                          \n",
    "    ),\n",
    "    input_dimension=input_dimension,\n",
    "    feature_dimension=128,\n",
    "    number_of_classes=number_of_classes,\n",
    "    freeze_extractors=False,                        \n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1334b429-d09c-449d-b252-e9a471ad109b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the cascading model\n",
    "\n",
    "trained_lgct_model = train_cascading_model(model=lgct_model, train_loader=train_loader, val_loader=val_loader,\n",
    "                                    num_epochs=10, learning_rate=1e-3, weight_decay=1e-5, \n",
    "                                    save_best_model_path=\"./permutations/LSTM_GNN_CNN_Transformer.pt\", use_scheduler=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c6d87e-3b0f-47f9-8d0b-4d395c16bb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the cascading model\n",
    "\n",
    "lgct_metrics = evaluate_model(model=trained_lgct_model, data_loader=test_loader, model_name=\"LGCT\",\n",
    "                         class_names=class_names, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e66e98-3081-4250-afef-648480c6b175",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgtc_model = build_intrusion_cascading_model(\n",
    "    model_sequence=generate_model_sequence(\n",
    "        perm=(\"LSTM\", \"GNN\", \"Transformer\", \"CNN\"), \n",
    "        model_builders=model_builders,               \n",
    "        input_dimension=input_dimension,                         \n",
    "        feature_dimension=128,                       \n",
    "        number_of_classes=number_of_classes                          \n",
    "    ),\n",
    "    input_dimension=input_dimension,\n",
    "    feature_dimension=128,\n",
    "    number_of_classes=number_of_classes,\n",
    "    freeze_extractors=False,                        \n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2835c858-f67d-4e4f-b9bc-3ea0fa14b521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the cascading model\n",
    "\n",
    "trained_lgtc_model = train_cascading_model(model=lgtc_model, train_loader=train_loader, val_loader=val_loader,\n",
    "                                    num_epochs=10, learning_rate=1e-3, weight_decay=1e-5, \n",
    "                                    save_best_model_path=\"./permutations/LSTM_GNN_Transformer_CNN.pt\", use_scheduler=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d204ea-a571-4fc7-95ac-542ebe3ef7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the cascading model\n",
    "\n",
    "lgtc_metrics = evaluate_model(model=trained_lgtc_model, data_loader=test_loader, model_name=\"LGTC\",\n",
    "                         class_names=class_names, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabc6862-b241-46bb-ba2f-8bb962516b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tclg_model = build_intrusion_cascading_model(\n",
    "    model_sequence=generate_model_sequence(\n",
    "        perm=(\"Transformer\", \"CNN\", \"LSTM\", \"GNN\"),  \n",
    "        model_builders=model_builders,               \n",
    "        input_dimension=input_dimension,                          \n",
    "        feature_dimension=128,                       \n",
    "        number_of_classes=number_of_classes                          \n",
    "    ),\n",
    "    input_dimension=input_dimension,\n",
    "    feature_dimension=128,\n",
    "    number_of_classes=number_of_classes,\n",
    "    freeze_extractors=False,                        \n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd751272-020e-48a6-9e10-a5e9580b2120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the cascading model\n",
    "\n",
    "trained_tclg_model = train_cascading_model(model=tclg_model, train_loader=train_loader, val_loader=val_loader,\n",
    "                                    num_epochs=10, learning_rate=1e-3, weight_decay=1e-5, \n",
    "                                    save_best_model_path=\"./permutations/Transformer_CNN_LSTM_GNN.pt\", use_scheduler=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3646494-2d95-4168-aced-a60657c2e9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the cascading model\n",
    "\n",
    "tclg_metrics = evaluate_model(model=trained_tclg_model, data_loader=test_loader, model_name=\"TCLG\",\n",
    "                         class_names=class_names, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418cc2d0-88df-417b-b693-9c862ff38a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "tcgl_model = build_intrusion_cascading_model(\n",
    "    model_sequence=generate_model_sequence(\n",
    "        perm=(\"Transformer\", \"CNN\", \"GNN\", \"LSTM\"),  \n",
    "        model_builders=model_builders,               \n",
    "        input_dimension=input_dimension,                          \n",
    "        feature_dimension=128,                       \n",
    "        number_of_classes=number_of_classes                          \n",
    "    ),\n",
    "    input_dimension=input_dimension,\n",
    "    feature_dimension=128,\n",
    "    number_of_classes=number_of_classes,\n",
    "    freeze_extractors=False,                        \n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b48a68-136b-4c8a-9045-7767a298663a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the cascading model\n",
    "\n",
    "trained_tcgl_model = train_cascading_model(model=tcgl_model, train_loader=train_loader, val_loader=val_loader,\n",
    "                                    num_epochs=10, learning_rate=1e-3, weight_decay=1e-5, \n",
    "                                    save_best_model_path=\"./permutations/Transformer_CNN_GNN_LSTM.pt\", use_scheduler=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99caa9af-9b9f-4fca-8b22-a940d7972d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the cascading model\n",
    "\n",
    "tcgl_metrics = evaluate_model(model=trained_tcgl_model, data_loader=test_loader, model_name=\"TCGL\",\n",
    "                         class_names=class_names, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32896f9c-9ac2-4c07-8b0e-32797cc5d67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tlcg_model = build_intrusion_cascading_model(\n",
    "    model_sequence=generate_model_sequence(\n",
    "        perm=(\"Transformer\", \"LSTM\", \"CNN\", \"GNN\"),  \n",
    "        model_builders=model_builders,               \n",
    "        input_dimension=input_dimension,                          \n",
    "        feature_dimension=128,                       \n",
    "        number_of_classes=number_of_classes                          \n",
    "    ),\n",
    "    input_dimension=input_dimension,\n",
    "    feature_dimension=128,\n",
    "    number_of_classes=number_of_classes,\n",
    "    freeze_extractors=False,                        \n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8494b977-89a7-4cfd-8192-105bcde146c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the cascading model\n",
    "\n",
    "trained_tlcg_model = train_cascading_model(model=tlcg_model, train_loader=train_loader, val_loader=val_loader,\n",
    "                                    num_epochs=10, learning_rate=1e-3, weight_decay=1e-5, \n",
    "                                    save_best_model_path=\"./permutations/Transformer_LSTM_CNN_GNN.pt\", use_scheduler=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ffbcad-026d-46f5-9915-5918fa19eb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the cascading model\n",
    "\n",
    "tlcg_metrics = evaluate_model(model=trained_tlcg_model, data_loader=test_loader, model_name=\"TLCG\",\n",
    "                         class_names=class_names, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad29a5bb-692d-427b-8d6c-79979cb3ee9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tlgc_model = build_intrusion_cascading_model(\n",
    "    model_sequence=generate_model_sequence(\n",
    "        perm=(\"Transformer\", \"LSTM\", \"GNN\", \"CNN\"),  \n",
    "        model_builders=model_builders,               \n",
    "        input_dimension=input_dimension,                          \n",
    "        feature_dimension=128,                       \n",
    "        number_of_classes=number_of_classes                          \n",
    "    ),\n",
    "    input_dimension=input_dimension,\n",
    "    feature_dimension=128,\n",
    "    number_of_classes=number_of_classes,\n",
    "    freeze_extractors=False,                        \n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2195ddab-1c2a-4976-b48c-0286810b14f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the cascading model\n",
    "\n",
    "trained_tlgc_model = train_cascading_model(model=tlgc_model, train_loader=train_loader, val_loader=val_loader,\n",
    "                                    num_epochs=10, learning_rate=1e-3, weight_decay=1e-5, \n",
    "                                    save_best_model_path=\"./permutations/Transformer_LSTM_GNN_CNN.pt\", use_scheduler=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2a1c39-4bb1-4f82-9343-8f30e84c17c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the cascading model\n",
    "\n",
    "tlgc_metrics = evaluate_model(model=trained_tlgc_model, data_loader=test_loader, model_name=\"TLGC\",\n",
    "                         class_names=class_names, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00ad1ab-0555-4df2-affa-b888f24d575d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tglc_model = build_intrusion_cascading_model(\n",
    "    model_sequence=generate_model_sequence(\n",
    "        perm=(\"Transformer\", \"GNN\", \"CNN\", \"LSTM\"),  \n",
    "        model_builders=model_builders,               \n",
    "        input_dimension=input_dimension,                          \n",
    "        feature_dimension=128,                       \n",
    "        number_of_classes=number_of_classes                          \n",
    "    ),\n",
    "    input_dimension=input_dimension,\n",
    "    feature_dimension=128,\n",
    "    number_of_classes=number_of_classes,\n",
    "    freeze_extractors=False,                        \n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888c5d1d-f7f9-431c-a5ef-ff3ec7da2649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the cascading model\n",
    "\n",
    "trained_tgcl_model = train_cascading_model(model=tcgl_model, train_loader=train_loader, val_loader=val_loader,\n",
    "                                    num_epochs=10, learning_rate=1e-3, weight_decay=1e-5, \n",
    "                                    save_best_model_path=\"./permutations/Transformer_GNN_CNN_LSTM.pt\", use_scheduler=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc2afa6-6cb7-4844-9599-7b4c09828028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the cascading model\n",
    "\n",
    "tgcl_metrics = evaluate_model(model=trained_tgcl_model, data_loader=test_loader, model_name=\"TGCL\",\n",
    "                         class_names=class_names, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18919f4c-d62b-421e-9355-eddd460ef970",
   "metadata": {},
   "outputs": [],
   "source": [
    "tglc_model = build_intrusion_cascading_model(\n",
    "    model_sequence=generate_model_sequence(\n",
    "        perm=(\"Transformer\", \"GNN\", \"LSTM\", \"CNN\"),  \n",
    "        model_builders=model_builders,               \n",
    "        input_dimension=input_dimension,                          \n",
    "        feature_dimension=128,                       \n",
    "        number_of_classes=number_of_classes                          \n",
    "    ),\n",
    "    input_dimension=input_dimension,\n",
    "    feature_dimension=128,\n",
    "    number_of_classes=number_of_classes,\n",
    "    freeze_extractors=False,                        \n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f3e251-39ca-4adf-878e-c8ebf8403dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the cascading model\n",
    "\n",
    "trained_tglc_model = train_cascading_model(model=tglc_model, train_loader=train_loader, val_loader=val_loader,\n",
    "                                    num_epochs=10, learning_rate=1e-3, weight_decay=1e-5, \n",
    "                                    save_best_model_path=\"./permutations/Transformer_GNN_LSTM_CNN.pt\", use_scheduler=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c969a09e-47c1-4014-83dd-b7ea5b4258c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the cascading model\n",
    "\n",
    "tglc_metrics = evaluate_model(model=trained_tglc_model, data_loader=test_loader, model_name=\"TGLC\",\n",
    "                         class_names=class_names, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e132efc0-372e-4a67-91c9-910e5e10e640",
   "metadata": {},
   "outputs": [],
   "source": [
    "gclt_model = build_intrusion_cascading_model(\n",
    "    model_sequence=generate_model_sequence(\n",
    "        perm=(\"GNN\", \"CNN\", \"LSTM\", \"Transformer\"), \n",
    "        model_builders=model_builders,               \n",
    "        input_dimension=input_dimension,                        \n",
    "        feature_dimension=128,                       \n",
    "        number_of_classes=number_of_classes                          \n",
    "    ),\n",
    "    input_dimension=input_dimension,\n",
    "    feature_dimension=128,\n",
    "    number_of_classes=number_of_classes,\n",
    "    freeze_extractors=False,                         \n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62420d2-d29b-4212-aa69-f8421164d4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the cascading model\n",
    "\n",
    "trained_gclt_model = train_cascading_model(model=gclt_model, train_loader=train_loader, val_loader=val_loader,\n",
    "                                    num_epochs=10, learning_rate=1e-3, weight_decay=1e-5, \n",
    "                                    save_best_model_path=\"./permutations/GNN_CNN_LSTM_Transformer.pt\", use_scheduler=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a4d4cf-5b6f-4f5c-a583-49a8c2347e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the cascading model\n",
    "\n",
    "gclt_metrics = evaluate_model(model=trained_gclt_model, data_loader=test_loader, model_name=\"GCLT\",\n",
    "                         class_names=class_names, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c63e07-1524-49ca-878f-a6b5a7cd037a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gctl_model = build_intrusion_cascading_model(\n",
    "    model_sequence=generate_model_sequence(\n",
    "        perm=(\"GNN\", \"CNN\", \"Transformer\", \"LSTM\"), \n",
    "        model_builders=model_builders,               \n",
    "        input_dimension=input_dimension,                        \n",
    "        feature_dimension=128,                     \n",
    "        number_of_classes=number_of_classes                          \n",
    "    ),\n",
    "    input_dimension=input_dimension,\n",
    "    feature_dimension=128,\n",
    "    number_of_classes=number_of_classes,\n",
    "    freeze_extractors=False,                         \n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c458e64-d2db-4424-82ae-d56e8e8840e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the cascading model\n",
    "\n",
    "trained_gctl_model = train_cascading_model(model=gctl_model, train_loader=train_loader, val_loader=val_loader,\n",
    "                                    num_epochs=10, learning_rate=1e-3, weight_decay=1e-5, \n",
    "                                    save_best_model_path=\"./permutations/GNN_CNN_Transformer_LSTM.pt\", use_scheduler=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a729fed-1fce-43b6-b169-c104e95a4fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the cascading model\n",
    "\n",
    "gctl_metrics = evaluate_model(model=trained_gctl_model, data_loader=test_loader, model_name=\"GCTL\",\n",
    "                         class_names=class_names, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f605cd9-f83d-4ab3-81b6-1c14e1eb0b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "glct_model = build_intrusion_cascading_model(\n",
    "    model_sequence=generate_model_sequence(\n",
    "        perm=(\"GNN\", \"LSTM\", \"CNN\", \"Transformer\"),  \n",
    "        model_builders=model_builders,               \n",
    "        input_dimension=input_dimension,                          \n",
    "        feature_dimension=128,                       \n",
    "        number_of_classes=number_of_classes            \n",
    "    ),\n",
    "    input_dimension=input_dimension,\n",
    "    feature_dimension=128,\n",
    "    number_of_classes=number_of_classes,\n",
    "    freeze_extractors=False,                        \n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce865f19-c269-42e2-bfcb-b2e85f428924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the cascading model\n",
    "\n",
    "trained_glct_model = train_cascading_model(model=glct_model, train_loader=train_loader, val_loader=val_loader,\n",
    "                                    num_epochs=10, learning_rate=1e-3, weight_decay=1e-5, \n",
    "                                    save_best_model_path=\"./permutations/GNN_LSTM_CNN_Transformer.pt\", use_scheduler=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287a64c4-881a-4f81-a373-b4c5e4251966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the cascading model\n",
    "\n",
    "glct_metrics = evaluate_model(model=trained_glct_model, data_loader=test_loader, model_name=\"GLCT\",\n",
    "                         class_names=class_names, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93de990-07de-476d-8269-3ec35c47c327",
   "metadata": {},
   "outputs": [],
   "source": [
    "gltc_model = build_intrusion_cascading_model(\n",
    "    model_sequence=generate_model_sequence(\n",
    "        perm=(\"GNN\", \"LSTM\", \"Transformer\", \"CNN\"),  \n",
    "        model_builders=model_builders,               \n",
    "        input_dimension=input_dimension,                          \n",
    "        feature_dimension=128,                       \n",
    "        number_of_classes=number_of_classes                         \n",
    "    ),\n",
    "    input_dimension=input_dimension,\n",
    "    feature_dimension=128,\n",
    "    number_of_classes=number_of_classes,\n",
    "    freeze_extractors=False,                         \n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ea8faa-9fcf-489d-b41d-822f7ff1aab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the cascading model\n",
    "\n",
    "trained_gltc_model = train_cascading_model(model=gltc_model, train_loader=train_loader, val_loader=val_loader,\n",
    "                                    num_epochs=10, learning_rate=1e-3, weight_decay=1e-5, \n",
    "                                    save_best_model_path=\"./permutations/GNN_LSTM_Transformer_CNN.pt\", use_scheduler=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c52f10-61e7-45c8-af79-ffb778a3fa63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the cascading model\n",
    "\n",
    "gltc_metrics = evaluate_model(model=trained_gltc_model, data_loader=test_loader, model_name=\"GLTC\",\n",
    "                         class_names=class_names, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a874e14-4c4a-411e-976d-767a22a899e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "gtcl_model = build_intrusion_cascading_model(\n",
    "    model_sequence=generate_model_sequence(\n",
    "        perm=(\"GNN\", \"Transformer\", \"CNN\", \"LSTM\"),  \n",
    "        model_builders=model_builders,               \n",
    "        input_dimension=input_dimension,                          \n",
    "        feature_dimension=128,                       \n",
    "        number_of_classes=number_of_classes                        \n",
    "    ),\n",
    "    input_dimension=input_dimension,\n",
    "    feature_dimension=128,\n",
    "    number_of_classes=number_of_classes,\n",
    "    freeze_extractors=False,                       \n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb162c6c-27e9-4bf5-8c4f-cea8b4f763e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the cascading model\n",
    "\n",
    "trained_gtcl_model = train_cascading_model(model=gtcl_model, train_loader=train_loader, val_loader=val_loader,\n",
    "                                    num_epochs=10, learning_rate=1e-3, weight_decay=1e-5, \n",
    "                                    save_best_model_path=\"./permutations/GNN_Transformer_CNN_LSTM.pt\", use_scheduler=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e5c87c-aab0-47c3-ae65-fcf6378d36ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the cascading model\n",
    "\n",
    "gtcl_metrics = evaluate_model(model=trained_gtcl_model, data_loader=test_loader, model_name=\"GTCL\",\n",
    "                         class_names=class_names, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d051bf65-3e0a-4336-bd3a-713b8fbe5f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gtlc_model = build_intrusion_cascading_model(\n",
    "    model_sequence=generate_model_sequence(\n",
    "        perm=(\"GNN\",\"Transformer\", \"LSTM\", \"CNN\"),  \n",
    "        model_builders=model_builders,              \n",
    "        input_dimension=input_dimension,                         \n",
    "        feature_dimension=128,                       \n",
    "        number_of_classes=number_of_classes                         \n",
    "    ),\n",
    "    input_dimension=input_dimension,\n",
    "    feature_dimension=128,\n",
    "    number_of_classes=number_of_classes,\n",
    "    freeze_extractors=False,                        \n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47002ad-8809-4249-872c-27d9a57adb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Training the cascading model\n",
    "\n",
    "trained_gtlc_model = train_cascading_model(model=gtlc_model, train_loader=train_loader, val_loader=val_loader,\n",
    "                                    num_epochs=10, learning_rate=1e-3, weight_decay=1e-5, \n",
    "                                    save_best_model_path=\"./permutations/GNN_Transformer_LSTM_CNN.pt\", use_scheduler=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c98577d-c75f-494b-9baa-12f340150eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the cascading model\n",
    "\n",
    "gtlc_metrics = evaluate_model(model=trained_gtlc_model, data_loader=test_loader, model_name=\"GTLC\",\n",
    "                         class_names=class_names, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6473f63-832a-4fbc-b8d7-fd8489e19d97",
   "metadata": {},
   "source": [
    "========================  Saving the Cascading Models Metrics  ========================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38b4aec-491c-46af-ba56-4b4cef2bc8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cascading_metrics = {\"CLTG\": cltg_metrics, \"CLGT\": clgt_metrics, \"CTLG\": ctlg_metrics, \"CTGL\": ctgl_metrics,\n",
    "                           \"CGLT\": cglt_metrics, \"CGTL\": cgtl_metrics, \"LCTG\": lctg_metrics, \"LCGT\": lcgt_metrics,\n",
    "                           \"LTCG\": ltcg_metrics, \"LTGC\": ltgc_metrics, \"LGCT\": lgct_metrics, \"LGTC\": lgtc_metrics,\n",
    "                           \"TCLG\": tclg_metrics, \"TCGL\": tcgl_metrics, \"TLCG\": tlcg_metrics, \"TLGC\": tlgc_metrics,\n",
    "                           \"TGCL\": tgcl_metrics, \"TGLC\": tglc_metrics, \"GCLT\": gclt_metrics, \"GCTL\": gctl_metrics,\n",
    "                           \"GLCT\": glct_metrics, \"GLTC\": gltc_metrics, \"GTCL\": gltc_metrics, \"GTLC\": gtlc_metrics}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdc8f31-8903-4fc1-a282-c652f328cd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_the_cascading_metrics_to_csv(metrics_dict: dict, save_path: str):\n",
    "    \n",
    "    dataframe = pd.DataFrame([{\n",
    "        \"Model\": model_name,\n",
    "        \"Accuracy\": metrics['accuracy'],\n",
    "        \"Precision\": metrics['precision'],\n",
    "        \"Recall\": metrics['recall'],\n",
    "        \"F1 Score\": metrics['f1_score']\n",
    "    } for model_name, metrics in metrics_dict.items()])\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    base_path = os.path.splitext(save_path)[0]\n",
    "    excel_path = f\"{base_path}_{timestamp}.xlsx\"\n",
    "    csv_path = f\"{base_path}_{timestamp}.csv\"\n",
    "\n",
    "    dataframe.to_excel(excel_path, index=False)\n",
    "    dataframe.to_csv(csv_path, index=False)\n",
    "    \n",
    "    print(f\"ðŸ“Š Best standalone model metrics exported to:\\n- Excel: {excel_path}\\n- CSV: {csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3568241e-4267-4b62-9304-83fef8c45ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_the_cascading_metrics_to_csv(cascading_metrics, \"Cascading_permutations_metrics_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26a17b7-9195-415f-953c-f9e5bae16e39",
   "metadata": {},
   "source": [
    "===============================  Reloading the cascading models  ==============================  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "1d245ab4-bb10-45a4-857b-0c0de10259fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_cascading_models(directory: str, verbose: bool = True) -> list:\n",
    "    \"\"\"\n",
    "    Loads all fully saved PyTorch models from the directory.\n",
    "\n",
    "    Arguments:\n",
    "        directory (str): Path to directory containing model .pt files.\n",
    "\n",
    "    Returns:\n",
    "        list: List of loaded models.\n",
    "    \"\"\"\n",
    "    cascading_models_list = []\n",
    "\n",
    "    for file_name in sorted(os.listdir(directory)):\n",
    "        if file_name.endswith(\".pt\"):\n",
    "            model_path = os.path.join(directory, file_name)\n",
    "\n",
    "            # Loading full model directly\n",
    "            model = torch.load(model_path)\n",
    "\n",
    "            # Appending to list\n",
    "            cascading_models_list.append(model)\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"âœ… Loaded full model from {file_name}\")\n",
    "\n",
    "    return cascading_models_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152a722f-9f21-4a76-829d-d22cdc5e2900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the path to the saved models directory\n",
    "\n",
    "saving_directory = \"permutations\"\n",
    "\n",
    "cascading_models_list = load_all_cascading_models(directory=saving_directory, verbose=True)\n",
    "\n",
    "# Confirming how many models were loaded\n",
    "\n",
    "print(f\"Loaded {len(cascading_models_list)} models from '{saving_directory}' directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df93f50-ae95-4ca9-947b-cc4e0959b2a8",
   "metadata": {},
   "source": [
    "#### ====================  Clovis Mushagalusa CIRUBAKADERHA  ===================="
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env1",
   "language": "python",
   "name": "env1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
